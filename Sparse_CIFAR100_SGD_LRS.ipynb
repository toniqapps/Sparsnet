{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sparse_CIFAR100_SGD_LRS.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "zmTl2xZ7Aom6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##SparseNet models for Keras.\n",
        "### Reference\n",
        "- [Sparsely Connected Convolutional Networks](https://arxiv.org/abs/1801.05895)\n",
        "- [Github](https://github.com/lyken17/sparsenet)"
      ]
    },
    {
      "metadata": {
        "id": "gmG9fMSQa1Uc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Skip Connections:\n",
        "---\n",
        "<div align=\"justify\">Predicting detail information of complicated visual scene may require understanding it at multiple levels abstraction, from edges and textures to object categories. As we all know convolution neural network learns increasingly abstract visual representations when going to deeper layers. But training such  deep networks requires back-propogating a signal through all the layers of the given networks which results in loss at the end of network to be noisier due to deeper layers and it becomes worst has we go deep and we also need to store and maintain a feature computed early that network need to reuse, to overcome this we use skip connections which connect multiple outputs from different layer to $l_{th}$ layer which can provide a pathway for assembling  feature that  combines many level of abstraction</div>"
      ]
    },
    {
      "metadata": {
        "id": "y1-v-Vqsa3V4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Optimizer:\n",
        "___\n",
        "\n",
        "The choice of optimization algorithm indicates how fast  and optimum is your model.  To train a model we need to reduce the loss which is a functon of weight and bias. With the help of back propogation we back propogate the current error in the previous layer and modify the weights and bias such a way that the error is reduced. To modify weights we use optimization algorithm.\n",
        "\n",
        "Optimization function is usually use to calculate gradiant i.e. the partial derivative of loss function with respect to weights and weghts are modified in opposite direction of calculate gradient. This i cycle is repeated until we reach minima of loss function\n",
        "\n",
        "To reduce the loss and modify the weights we use hyper parameter learning rate which can be tuned to get optimal results. Choosing a proper learning rate is difficult. If the learning rate is too low it result in slow convergence and may lead to vanishing gradient descent  and if it is to high it may diverge from the minmal and may leading exploding gradient problem\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "lz5t9QXqa494",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Why Sparsenet : \n",
        "___\n",
        "<div align=\"justify\">SparseNet is a variant of DenseNets or Resnet. In Densenet we  have a skip connection after every block which are concatenated to next layer and in resnet we do cumulative summation, but both have same problems i.e. as the depth increases, the number of features grows linearly. Later features may corrupt or wash out the information carried by earlier features maps as seen in resnet which result in saturation of resnet performance, in contrast densnet preserves the original format of previous layers due to concatenation, this factor contribute to better parameter performance efficiency over resnet but due to concatenation no of parameters grows at the rate of $0(N^2)$ due to which portion of network is devoted to process previously seen feature map and hence are not able to exploit all the parameters fully and this pitfall are due to the linear growth of feature maps in both densnet and resnet. To overcome this we would like to maintain the power of short gradient paths for training. By aggregating features only from layers with exponential offset the length of the shortest  gradient path between blocks with offset S is bounded  by  $O((c-1)log(S))$. Here, c is again the base of the exponent governing the sparse connection pattern. The total number of output to the $l^{th}$ block is $O(log(l))$ due to exponential offsets. Therefore total no of skip connections is</div>\n",
        "\n",
        "<center>$\\sum_{l=1}^N(log_cl) = O(NlogN)$</center>\n",
        "\n",
        "<center>N is the number of basic blocks (depth) of the network. </center>\n",
        "\n",
        "<center>The number of parameters are $O(N log N)$ and $O(N)$, respectively, for aggregation by concatenation and aggregation by summation</center>\n",
        "\n",
        "<div align=\"justify\">SparseNets have such skip connections only at depths of $2^N$. This allows model to be less memory intensive due to less parameters while still performing equivalent to densnet or better</div>\n",
        "\n",
        "<center>![alt text](https://lh3.googleusercontent.com/ar5begWFXAGXPVDeIORZB_iD4OrsAe6dR-yyfEjCNhR8fnt-LnnFcRDUrecj7era4845nS8iyolaWmN0GaTCo114I9WmTSTo0cTIGBQnVwzvJ9yrVa0Fm0TYUnxphcHbQC5pAoWe=w2400)</center>\n",
        "\n",
        "<center>** Densnet/Resnet**</center>\n",
        "\n",
        "<center>![alt text](https://lh3.googleusercontent.com/pJosUXvPpuMJu87oi0gb351VelsWpLkbcX6TXx5i1qh_QOaMEPgeJS-Ikg3Dilfue6qDNnfOblaOpc8BUJOzgY4yPE23QxOBttS268ojfYJajR7uBGg__cNisOUUIp0f-vNfx7zi=w2400)</center>\n",
        "\n",
        "<center>![alt text](https://lh3.googleusercontent.com/EnnfenqM7PICFgtcfxgVO0PWJOCbpFFCCq0DDSRhBoZU63ZgZUzqAsGWx0tSZCbULSNwqfBDEr41Q0enZUJXAUON3j2s30aqosQZrrsgBHWHjWJpB4Xo5bBlm-NvmBkkXJNOTRMp=w2400)</center>\n",
        "<center>** Sparsenet**</center>\n"
      ]
    },
    {
      "metadata": {
        "id": "fPkwdWE6a7jG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Experiments\n",
        "\n",
        "<div align=\"justify\">We demonstrate the effectiveness of SparseNets over DenseNets, through image classification tasks on the CIFAR-100 datasets</div>\n",
        "\n",
        "We Implement our models in Keras\n",
        "\n",
        "**Datasets**:\n",
        "CIFAR both the CIFAR-10 and CIFAR-100 datasets have 50,000 training\n",
        "images and 10,000 testing images with size of 32 Ã— 32 pixels. CIFAR-10 \n",
        "and CIFAR-100  have 10 and 100 classes respectively. Our experiments\n",
        "use standard data augmentation"
      ]
    },
    {
      "metadata": {
        "id": "iYZh5OwzAkql",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import warnings\n",
        "from scipy.misc import imresize, toimage\n",
        "\n",
        "from keras.models import Model\n",
        "from keras.layers.core import Dense, Dropout, Activation, Reshape\n",
        "from keras.layers.convolutional import Conv2D, Conv2DTranspose, UpSampling2D, SeparableConv2D\n",
        "from keras.layers import AveragePooling2D, MaxPooling2D\n",
        "from keras.layers import GlobalAveragePooling2D\n",
        "from keras.layers import Input\n",
        "from keras.layers.merge import concatenate\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.regularizers import l2\n",
        "from keras.utils.layer_utils import convert_all_kernels_in_model, convert_dense_weights_data_format\n",
        "from keras.utils.data_utils import get_file\n",
        "from keras.engine.topology import get_source_inputs\n",
        "from keras.applications.imagenet_utils import _obtain_input_shape\n",
        "from keras.applications.imagenet_utils import decode_predictions\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "import keras.backend as K\n",
        "\n",
        "\n",
        "import os.path\n",
        "import sklearn.metrics as metrics\n",
        "from keras.datasets import cifar100\n",
        "from keras.utils import np_utils\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.optimizers import SGD\n",
        "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1Qmhj1pqbEui",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Load Datasets"
      ]
    },
    {
      "metadata": {
        "id": "bCrrJRnE90r_",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#load data\n",
        "(trainX, trainY), (testX, testY) = cifar100.load_data()\n",
        "\n",
        "trainX = trainX.astype('float32')\n",
        "testX = testX.astype('float32')\n",
        "\n",
        "cifar_mean = trainX.mean(axis=(0, 1, 2), keepdims=True)\n",
        "cifar_std = trainX.std(axis=(0, 1, 2), keepdims=True)\n",
        "\n",
        "trainX = (trainX - cifar_mean) / (cifar_std + 1e-8)\n",
        "testX = (testX - cifar_mean) / (cifar_std + 1e-8)\n",
        "\n",
        "Y_train = np_utils.to_categorical(trainY, nb_classes)\n",
        "Y_test = np_utils.to_categorical(testY, nb_classes)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YBFVEC1rbK3z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Hyper Parameters\n"
      ]
    },
    {
      "metadata": {
        "id": "rJ48Th6KbRKQ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "dropout_rate = 0.0\n",
        "growth_rate=24\n",
        "compression = 0.5\n",
        "depth = 40\n",
        "bottleneck=False\n",
        "weight_decay=1e-4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T9YY3JgFbVqA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Batch Normalization** : Use to normalize output of previous activation layer\n",
        "\n",
        "**Relu** : Use to convert all the negative values to zero and positive values unchanged\n",
        "\n",
        "**Conv2D**: Apply kernel with receptive field size of 3x3 with bias zero and padding same to maintain output feature map same as input feature map\n",
        "\n",
        "**Dropout**: Reducing overfiitting by dropping few node randomly\n",
        "\n",
        "**Bottleneck Layer** : To improve computational efficiency we can introduce bottleneck layer 3x3 convolution layer to reduce the no of feature maps\n",
        "\n",
        "**Compression**:  We can also reduce feature maps in transition layer by introducing compression factor whose value lies between $0<\\theta\\le1$"
      ]
    },
    {
      "metadata": {
        "id": "CEalEZ86baVj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Exponential $2^N$\n",
        "We consider skip connections only at depths of $2^N$.  \n",
        "\n",
        "**input** : List of processed layers\n",
        "\n",
        "**returns** : layers which are exponential of $2^N$"
      ]
    },
    {
      "metadata": {
        "id": "JzItM4I2BRr4",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def _exponential_index_fetch(x_list):\n",
        "    count = len(x_list)\n",
        "    i = 1\n",
        "    inputs = []\n",
        "    while i <= count:\n",
        "        inputs.append(x_list[count - i])\n",
        "        i *= 2\n",
        "    return inputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OQuWk5LibNpL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Convolution Layer with &amp; Wihtout Bottleneck Layer\n",
        "\n",
        "###Parameters:\n",
        "\n",
        "**Input**: previous layer output (ip), num of filter (nb_filter)\n",
        "\n",
        "**Output** : current layer output (x)\n",
        "\n",
        "**note**: Both bottleneck layer and dropout are conditional "
      ]
    },
    {
      "metadata": {
        "id": "5ulmYZkCBZjz",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def add_conv_block(ip, nb_filter):\n",
        "    \n",
        "  concat_axis = -1\n",
        "\n",
        "  x = BatchNormalization(axis=concat_axis, momentum=0.1, epsilon=1e-5)(ip)\n",
        "  x = Activation('relu')(x)\n",
        "\n",
        "  if bottleneck:\n",
        "    # Obtained from https://github.com/liuzhuang13/DenseNet/blob/master/densenet.lua\n",
        "    inter_channel = nb_filter * 4 \n",
        "\n",
        "    x = Conv2D(inter_channel, (1, 1), kernel_initializer='he_normal', padding='same', use_bias=False,\n",
        "               kernel_regularizer=l2(weight_decay))(x)\n",
        "    x = BatchNormalization(axis=concat_axis, epsilon=1e-5, momentum=0.1)(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "  x = Conv2D(nb_filter, (3, 3), kernel_initializer='he_normal', padding='same', use_bias=False)(x)\n",
        "  if dropout_rate > 0.0:\n",
        "    x = Dropout(dropout_rate)(x)\n",
        "\n",
        "  return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3-wNgpFAbrFm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Dense Block\n",
        "\n",
        "###Parameters:\n",
        "\n",
        "**Input**: \n",
        "\n",
        "1.   previous layer output (ip)\n",
        "2.   no of layers per dense block (nb_layers)\n",
        "3. rate at which no of filter grow (growth_rate), grow filter exponentially               (grow_nb_filters)\n",
        "\n",
        "**Output** : \n",
        "1. current layer output (x) \n",
        "2. no of filter (nb_filter)\n",
        "\n",
        "###Explanation\n",
        "\n",
        "In this function we create layers based on no of layers (nb_layers)  and concatenate  output layers from previous layers  which are $2^N$  exponential and also grow no of filter exponentially \n"
      ]
    },
    {
      "metadata": {
        "id": "gHOTlQmjBctr",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def add_dense_block(x, nb_layers, nb_filter):\n",
        "    \n",
        "    concat_axis = -1\n",
        "\n",
        "    x_list = [x]\n",
        "    channel_list = [nb_filter]\n",
        "\n",
        "    for i in range(nb_layers):\n",
        "      x = add_conv_block(x, growth_rate)\n",
        "      x_list.append(x)\n",
        "\n",
        "      fetch_outputs = _exponential_index_fetch(x_list)\n",
        "      x = concatenate(fetch_outputs, axis=concat_axis)\n",
        "\n",
        "      channel_list.append(growth_rate)\n",
        "\n",
        "    nb_filter = sum(_exponential_index_fetch(channel_list))\n",
        "\n",
        "    return x, nb_filter"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2hfwbwIibyww",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Transition Layer\n",
        "\n",
        "\n",
        "###Parameters:\n",
        "\n",
        "**Input**: \n",
        "\n",
        "1.   previous layer output (ip)\n",
        "2. no of filters (nb_filter)\n",
        "3. reduction ratio of transition layer (compression)\n",
        "\n",
        "**Output** : \n",
        "1. current layer output (x) \n",
        "\n",
        "###Explanation\n",
        "\n",
        "Use to reduce the no of feature maps after each dense block. This is use to control no of parameters that flow to next dense block for better computancy. Using compression we control the reduction ratio of transition layerdef add_transition_block(ip, nb_filter):\n",
        "  concat_axis = -1\n",
        "  x = BatchNormalization(axis=concat_axis, epsilon=1e-5, momentum=0.1)(ip)\n",
        "  x = Activation('relu')(x)\n",
        "  x = Conv2D(int(nb_filter * compression), (1, 1), kernel_initializer='he_normal', padding='same', use_bias=False,\n",
        "             kernel_regularizer=l2(weight_decay))(x)\n",
        "  x = AveragePooling2D((2, 2))(x)\n",
        "\n",
        "  return x"
      ]
    },
    {
      "metadata": {
        "id": "o1LKpdeQBg9G",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def add_transition_block(ip, nb_filter):\n",
        "  concat_axis = -1\n",
        "  x = BatchNormalization(axis=concat_axis, epsilon=1e-5, momentum=0.1)(ip)\n",
        "  x = Activation('relu')(x)\n",
        "  x = Conv2D(int(nb_filter * compression), (1, 1), kernel_initializer='he_normal', padding='same', use_bias=False,\n",
        "             kernel_regularizer=l2(weight_decay))(x)\n",
        "  x = AveragePooling2D((2, 2))(x)\n",
        "\n",
        "  return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0G1fi6s-b3v3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Create Sparsenet\n",
        "\n",
        "\n",
        "###Parameters:\n",
        "\n",
        "**Input**: \n",
        "\n",
        "1.   Image Input (img_input)\n",
        "2.   total no of layer in the given sparsenet model (depth)\n",
        "4. growth rate\n",
        "\n",
        "**Output** : \n",
        "1. model output (x) \n",
        "\n",
        "###Explanation\n",
        "\n",
        "Based on given depth we divide total no layers into layers in dense block, layer in transition layer and layer in bottleneck layer using the below formula we divide the layers into the given blocks and layers\n",
        "<center>$Depth = 3N+4$</center>\n",
        "**note**: by default we consider 3 dense block and last dense block doesn't have transition layer\n",
        "\n",
        "We convert given no of dense block in our case 3 into a list and also growth rate into list remember both lost should be of dense block size\n",
        "\n",
        "We assign nb_filter = growth rate for firstlayer which is a convolution layer of kerner receptive field size of 3x3 following this layers is our 3 dense block and 2 transition layer and bottleneck layers if true and finall y the output layer"
      ]
    },
    {
      "metadata": {
        "id": "Bgj-OFe3BlUD",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def create_dense_net(img_input):\n",
        "   \n",
        "    global compression\n",
        "    #channel_last\n",
        "    concat_axis = -1\n",
        "    #no of dense_block\n",
        "    nb_dense_block=3\n",
        "\n",
        "    # layers in each dense block\n",
        "    assert (depth - 4) % nb_dense_block == 0, 'Depth must be 3 N + 4'\n",
        "    count = int((depth - 4) / nb_dense_block)\n",
        "\n",
        "    if bottleneck:\n",
        "      count = count // 2\n",
        "    else:\n",
        "      compression = 1\n",
        "\n",
        "    #convert int list\n",
        "    nb_layers = [count for _ in range(nb_dense_block)]\n",
        "    final_nb_layer = count\n",
        "\n",
        "    # compute initial nb_filter\n",
        "    nb_filter = 2 * growth_rate\n",
        "\n",
        "    # Initial convolution\n",
        "    x = Conv2D(nb_filter, (3,3), kernel_initializer='he_normal', padding='same', use_bias=False, kernel_regularizer=l2(weight_decay))(img_input)\n",
        "\n",
        "    # Add dense blocks\n",
        "    for i in range(nb_dense_block - 1):\n",
        "        # add dense block\n",
        "        x, nb_filter = add_dense_block(x, nb_layers[i], nb_filter)\n",
        "        # add transition_block\n",
        "        x = add_transition_block(x, nb_filter)\n",
        "        nb_filter = int(nb_filter * compression)\n",
        "\n",
        "    # The last dense_block does not have a transition_block\n",
        "    x, nb_filter = add_dense_block(x, final_nb_layer, nb_filter)\n",
        "\n",
        "    x = BatchNormalization(axis=concat_axis, epsilon=1e-5, momentum=0.1)(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "\n",
        "    x = Dense(nb_classes, activation='softmax')(x)\n",
        "\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ul1vCnX2b9mw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Create Sparse Model\n",
        "###parameters\n",
        "**batch_size** :  The datasets which we use is cifar and consist of 50000 train and 10000 test images which we divide into no of batches with each batch size equal to 64 and total no of batches forms 1 epoch"
      ]
    },
    {
      "metadata": {
        "id": "y511YpgB6C0S",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def SparseNet(input_shape):\n",
        "    \n",
        "    inputs = Input(shape=input_shape)\n",
        "    \n",
        "    output = create_dense_net(inputs)\n",
        "        \n",
        "    # Create model.\n",
        "    model = Model(inputs, output)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2JI6rRGiIgd8",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "nb_classes = 100\n",
        "epoch = 150\n",
        "nb_epoch_24 = 15\n",
        "nb_epoch_32 = 135\n",
        "\n",
        "img_rows, img_cols = None, None\n",
        "img_channels = 3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wGFtCLK96NJl",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 6953
        },
        "outputId": "9aad9c61-d79d-4dd2-9fe9-dce4de63b014",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1530011288018,
          "user_tz": -330,
          "elapsed": 5901,
          "user": {
            "displayName": "Rahul Jain",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "117177615195402907037"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "model = SparseNet((None,None,nb_channels))\n",
        "print(\"Model created\")\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model created\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            (None, None, None, 3 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_40 (Conv2D)              (None, None, None, 4 1296        input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_40 (BatchNo (None, None, None, 4 192         conv2d_40[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_40 (Activation)      (None, None, None, 4 0           batch_normalization_40[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_41 (Conv2D)              (None, None, None, 2 10368       activation_40[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_37 (Concatenate)    (None, None, None, 7 0           conv2d_41[0][0]                  \n",
            "                                                                 conv2d_40[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_41 (BatchNo (None, None, None, 7 288         concatenate_37[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_41 (Activation)      (None, None, None, 7 0           batch_normalization_41[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_42 (Conv2D)              (None, None, None, 2 15552       activation_41[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_38 (Concatenate)    (None, None, None, 4 0           conv2d_42[0][0]                  \n",
            "                                                                 conv2d_41[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_42 (BatchNo (None, None, None, 4 192         concatenate_38[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_42 (Activation)      (None, None, None, 4 0           batch_normalization_42[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_43 (Conv2D)              (None, None, None, 2 10368       activation_42[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_39 (Concatenate)    (None, None, None, 9 0           conv2d_43[0][0]                  \n",
            "                                                                 conv2d_42[0][0]                  \n",
            "                                                                 conv2d_40[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_43 (BatchNo (None, None, None, 9 384         concatenate_39[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_43 (Activation)      (None, None, None, 9 0           batch_normalization_43[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_44 (Conv2D)              (None, None, None, 2 20736       activation_43[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_40 (Concatenate)    (None, None, None, 7 0           conv2d_44[0][0]                  \n",
            "                                                                 conv2d_43[0][0]                  \n",
            "                                                                 conv2d_41[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_44 (BatchNo (None, None, None, 7 288         concatenate_40[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_44 (Activation)      (None, None, None, 7 0           batch_normalization_44[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_45 (Conv2D)              (None, None, None, 2 15552       activation_44[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_41 (Concatenate)    (None, None, None, 7 0           conv2d_45[0][0]                  \n",
            "                                                                 conv2d_44[0][0]                  \n",
            "                                                                 conv2d_42[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_45 (BatchNo (None, None, None, 7 288         concatenate_41[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_45 (Activation)      (None, None, None, 7 0           batch_normalization_45[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_46 (Conv2D)              (None, None, None, 2 15552       activation_45[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_42 (Concatenate)    (None, None, None, 7 0           conv2d_46[0][0]                  \n",
            "                                                                 conv2d_45[0][0]                  \n",
            "                                                                 conv2d_43[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_46 (BatchNo (None, None, None, 7 288         concatenate_42[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_46 (Activation)      (None, None, None, 7 0           batch_normalization_46[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_47 (Conv2D)              (None, None, None, 2 15552       activation_46[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_43 (Concatenate)    (None, None, None, 1 0           conv2d_47[0][0]                  \n",
            "                                                                 conv2d_46[0][0]                  \n",
            "                                                                 conv2d_44[0][0]                  \n",
            "                                                                 conv2d_40[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_47 (BatchNo (None, None, None, 1 480         concatenate_43[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_47 (Activation)      (None, None, None, 1 0           batch_normalization_47[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_48 (Conv2D)              (None, None, None, 2 25920       activation_47[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_44 (Concatenate)    (None, None, None, 9 0           conv2d_48[0][0]                  \n",
            "                                                                 conv2d_47[0][0]                  \n",
            "                                                                 conv2d_45[0][0]                  \n",
            "                                                                 conv2d_41[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_48 (BatchNo (None, None, None, 9 384         concatenate_44[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_48 (Activation)      (None, None, None, 9 0           batch_normalization_48[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_49 (Conv2D)              (None, None, None, 2 20736       activation_48[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_45 (Concatenate)    (None, None, None, 9 0           conv2d_49[0][0]                  \n",
            "                                                                 conv2d_48[0][0]                  \n",
            "                                                                 conv2d_46[0][0]                  \n",
            "                                                                 conv2d_42[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_49 (BatchNo (None, None, None, 9 384         concatenate_45[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_49 (Activation)      (None, None, None, 9 0           batch_normalization_49[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_50 (Conv2D)              (None, None, None, 2 20736       activation_49[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_46 (Concatenate)    (None, None, None, 9 0           conv2d_50[0][0]                  \n",
            "                                                                 conv2d_49[0][0]                  \n",
            "                                                                 conv2d_47[0][0]                  \n",
            "                                                                 conv2d_43[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_50 (BatchNo (None, None, None, 9 384         concatenate_46[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_50 (Activation)      (None, None, None, 9 0           batch_normalization_50[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_51 (Conv2D)              (None, None, None, 2 20736       activation_50[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_47 (Concatenate)    (None, None, None, 9 0           conv2d_51[0][0]                  \n",
            "                                                                 conv2d_50[0][0]                  \n",
            "                                                                 conv2d_48[0][0]                  \n",
            "                                                                 conv2d_44[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_51 (BatchNo (None, None, None, 9 384         concatenate_47[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_51 (Activation)      (None, None, None, 9 0           batch_normalization_51[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_52 (Conv2D)              (None, None, None, 2 20736       activation_51[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_48 (Concatenate)    (None, None, None, 9 0           conv2d_52[0][0]                  \n",
            "                                                                 conv2d_51[0][0]                  \n",
            "                                                                 conv2d_49[0][0]                  \n",
            "                                                                 conv2d_45[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_52 (BatchNo (None, None, None, 9 384         concatenate_48[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_52 (Activation)      (None, None, None, 9 0           batch_normalization_52[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_53 (Conv2D)              (None, None, None, 9 9216        activation_52[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_3 (AveragePoo (None, None, None, 9 0           conv2d_53[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_53 (BatchNo (None, None, None, 9 384         average_pooling2d_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_53 (Activation)      (None, None, None, 9 0           batch_normalization_53[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_54 (Conv2D)              (None, None, None, 2 20736       activation_53[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_49 (Concatenate)    (None, None, None, 1 0           conv2d_54[0][0]                  \n",
            "                                                                 average_pooling2d_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_54 (BatchNo (None, None, None, 1 480         concatenate_49[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_54 (Activation)      (None, None, None, 1 0           batch_normalization_54[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_55 (Conv2D)              (None, None, None, 2 25920       activation_54[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_50 (Concatenate)    (None, None, None, 4 0           conv2d_55[0][0]                  \n",
            "                                                                 conv2d_54[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_55 (BatchNo (None, None, None, 4 192         concatenate_50[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_55 (Activation)      (None, None, None, 4 0           batch_normalization_55[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_56 (Conv2D)              (None, None, None, 2 10368       activation_55[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_51 (Concatenate)    (None, None, None, 1 0           conv2d_56[0][0]                  \n",
            "                                                                 conv2d_55[0][0]                  \n",
            "                                                                 average_pooling2d_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_56 (BatchNo (None, None, None, 1 576         concatenate_51[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_56 (Activation)      (None, None, None, 1 0           batch_normalization_56[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_57 (Conv2D)              (None, None, None, 2 31104       activation_56[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_52 (Concatenate)    (None, None, None, 7 0           conv2d_57[0][0]                  \n",
            "                                                                 conv2d_56[0][0]                  \n",
            "                                                                 conv2d_54[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_57 (BatchNo (None, None, None, 7 288         concatenate_52[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_57 (Activation)      (None, None, None, 7 0           batch_normalization_57[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_58 (Conv2D)              (None, None, None, 2 15552       activation_57[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_53 (Concatenate)    (None, None, None, 7 0           conv2d_58[0][0]                  \n",
            "                                                                 conv2d_57[0][0]                  \n",
            "                                                                 conv2d_55[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_58 (BatchNo (None, None, None, 7 288         concatenate_53[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_58 (Activation)      (None, None, None, 7 0           batch_normalization_58[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_59 (Conv2D)              (None, None, None, 2 15552       activation_58[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_54 (Concatenate)    (None, None, None, 7 0           conv2d_59[0][0]                  \n",
            "                                                                 conv2d_58[0][0]                  \n",
            "                                                                 conv2d_56[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_59 (BatchNo (None, None, None, 7 288         concatenate_54[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_59 (Activation)      (None, None, None, 7 0           batch_normalization_59[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_60 (Conv2D)              (None, None, None, 2 15552       activation_59[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_55 (Concatenate)    (None, None, None, 1 0           conv2d_60[0][0]                  \n",
            "                                                                 conv2d_59[0][0]                  \n",
            "                                                                 conv2d_57[0][0]                  \n",
            "                                                                 average_pooling2d_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_60 (BatchNo (None, None, None, 1 672         concatenate_55[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_60 (Activation)      (None, None, None, 1 0           batch_normalization_60[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_61 (Conv2D)              (None, None, None, 2 36288       activation_60[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_56 (Concatenate)    (None, None, None, 9 0           conv2d_61[0][0]                  \n",
            "                                                                 conv2d_60[0][0]                  \n",
            "                                                                 conv2d_58[0][0]                  \n",
            "                                                                 conv2d_54[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_61 (BatchNo (None, None, None, 9 384         concatenate_56[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_61 (Activation)      (None, None, None, 9 0           batch_normalization_61[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_62 (Conv2D)              (None, None, None, 2 20736       activation_61[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_57 (Concatenate)    (None, None, None, 9 0           conv2d_62[0][0]                  \n",
            "                                                                 conv2d_61[0][0]                  \n",
            "                                                                 conv2d_59[0][0]                  \n",
            "                                                                 conv2d_55[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_62 (BatchNo (None, None, None, 9 384         concatenate_57[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_62 (Activation)      (None, None, None, 9 0           batch_normalization_62[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_63 (Conv2D)              (None, None, None, 2 20736       activation_62[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_58 (Concatenate)    (None, None, None, 9 0           conv2d_63[0][0]                  \n",
            "                                                                 conv2d_62[0][0]                  \n",
            "                                                                 conv2d_60[0][0]                  \n",
            "                                                                 conv2d_56[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_63 (BatchNo (None, None, None, 9 384         concatenate_58[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_63 (Activation)      (None, None, None, 9 0           batch_normalization_63[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_64 (Conv2D)              (None, None, None, 2 20736       activation_63[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_59 (Concatenate)    (None, None, None, 9 0           conv2d_64[0][0]                  \n",
            "                                                                 conv2d_63[0][0]                  \n",
            "                                                                 conv2d_61[0][0]                  \n",
            "                                                                 conv2d_57[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_64 (BatchNo (None, None, None, 9 384         concatenate_59[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_64 (Activation)      (None, None, None, 9 0           batch_normalization_64[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_65 (Conv2D)              (None, None, None, 2 20736       activation_64[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_60 (Concatenate)    (None, None, None, 9 0           conv2d_65[0][0]                  \n",
            "                                                                 conv2d_64[0][0]                  \n",
            "                                                                 conv2d_62[0][0]                  \n",
            "                                                                 conv2d_58[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_65 (BatchNo (None, None, None, 9 384         concatenate_60[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_65 (Activation)      (None, None, None, 9 0           batch_normalization_65[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_66 (Conv2D)              (None, None, None, 9 9216        activation_65[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_4 (AveragePoo (None, None, None, 9 0           conv2d_66[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_66 (BatchNo (None, None, None, 9 384         average_pooling2d_4[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_66 (Activation)      (None, None, None, 9 0           batch_normalization_66[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_67 (Conv2D)              (None, None, None, 2 20736       activation_66[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_61 (Concatenate)    (None, None, None, 1 0           conv2d_67[0][0]                  \n",
            "                                                                 average_pooling2d_4[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_67 (BatchNo (None, None, None, 1 480         concatenate_61[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_67 (Activation)      (None, None, None, 1 0           batch_normalization_67[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_68 (Conv2D)              (None, None, None, 2 25920       activation_67[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_62 (Concatenate)    (None, None, None, 4 0           conv2d_68[0][0]                  \n",
            "                                                                 conv2d_67[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_68 (BatchNo (None, None, None, 4 192         concatenate_62[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_68 (Activation)      (None, None, None, 4 0           batch_normalization_68[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_69 (Conv2D)              (None, None, None, 2 10368       activation_68[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_63 (Concatenate)    (None, None, None, 1 0           conv2d_69[0][0]                  \n",
            "                                                                 conv2d_68[0][0]                  \n",
            "                                                                 average_pooling2d_4[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_69 (BatchNo (None, None, None, 1 576         concatenate_63[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_69 (Activation)      (None, None, None, 1 0           batch_normalization_69[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_70 (Conv2D)              (None, None, None, 2 31104       activation_69[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_64 (Concatenate)    (None, None, None, 7 0           conv2d_70[0][0]                  \n",
            "                                                                 conv2d_69[0][0]                  \n",
            "                                                                 conv2d_67[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_70 (BatchNo (None, None, None, 7 288         concatenate_64[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_70 (Activation)      (None, None, None, 7 0           batch_normalization_70[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_71 (Conv2D)              (None, None, None, 2 15552       activation_70[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_65 (Concatenate)    (None, None, None, 7 0           conv2d_71[0][0]                  \n",
            "                                                                 conv2d_70[0][0]                  \n",
            "                                                                 conv2d_68[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_71 (BatchNo (None, None, None, 7 288         concatenate_65[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_71 (Activation)      (None, None, None, 7 0           batch_normalization_71[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_72 (Conv2D)              (None, None, None, 2 15552       activation_71[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_66 (Concatenate)    (None, None, None, 7 0           conv2d_72[0][0]                  \n",
            "                                                                 conv2d_71[0][0]                  \n",
            "                                                                 conv2d_69[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_72 (BatchNo (None, None, None, 7 288         concatenate_66[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_72 (Activation)      (None, None, None, 7 0           batch_normalization_72[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_73 (Conv2D)              (None, None, None, 2 15552       activation_72[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_67 (Concatenate)    (None, None, None, 1 0           conv2d_73[0][0]                  \n",
            "                                                                 conv2d_72[0][0]                  \n",
            "                                                                 conv2d_70[0][0]                  \n",
            "                                                                 average_pooling2d_4[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_73 (BatchNo (None, None, None, 1 672         concatenate_67[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_73 (Activation)      (None, None, None, 1 0           batch_normalization_73[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_74 (Conv2D)              (None, None, None, 2 36288       activation_73[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_68 (Concatenate)    (None, None, None, 9 0           conv2d_74[0][0]                  \n",
            "                                                                 conv2d_73[0][0]                  \n",
            "                                                                 conv2d_71[0][0]                  \n",
            "                                                                 conv2d_67[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_74 (BatchNo (None, None, None, 9 384         concatenate_68[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_74 (Activation)      (None, None, None, 9 0           batch_normalization_74[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_75 (Conv2D)              (None, None, None, 2 20736       activation_74[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_69 (Concatenate)    (None, None, None, 9 0           conv2d_75[0][0]                  \n",
            "                                                                 conv2d_74[0][0]                  \n",
            "                                                                 conv2d_72[0][0]                  \n",
            "                                                                 conv2d_68[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_75 (BatchNo (None, None, None, 9 384         concatenate_69[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_75 (Activation)      (None, None, None, 9 0           batch_normalization_75[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_76 (Conv2D)              (None, None, None, 2 20736       activation_75[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_70 (Concatenate)    (None, None, None, 9 0           conv2d_76[0][0]                  \n",
            "                                                                 conv2d_75[0][0]                  \n",
            "                                                                 conv2d_73[0][0]                  \n",
            "                                                                 conv2d_69[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_76 (BatchNo (None, None, None, 9 384         concatenate_70[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_76 (Activation)      (None, None, None, 9 0           batch_normalization_76[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_77 (Conv2D)              (None, None, None, 2 20736       activation_76[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_71 (Concatenate)    (None, None, None, 9 0           conv2d_77[0][0]                  \n",
            "                                                                 conv2d_76[0][0]                  \n",
            "                                                                 conv2d_74[0][0]                  \n",
            "                                                                 conv2d_70[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_77 (BatchNo (None, None, None, 9 384         concatenate_71[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_77 (Activation)      (None, None, None, 9 0           batch_normalization_77[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_78 (Conv2D)              (None, None, None, 2 20736       activation_77[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_72 (Concatenate)    (None, None, None, 9 0           conv2d_78[0][0]                  \n",
            "                                                                 conv2d_77[0][0]                  \n",
            "                                                                 conv2d_75[0][0]                  \n",
            "                                                                 conv2d_71[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_78 (BatchNo (None, None, None, 9 384         concatenate_72[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_78 (Activation)      (None, None, None, 9 0           batch_normalization_78[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_2 (Glo (None, 96)           0           activation_78[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 100)          9700        global_average_pooling2d_2[0][0] \n",
            "==================================================================================================\n",
            "Total params: 764,500\n",
            "Trainable params: 757,252\n",
            "Non-trainable params: 7,248\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5UFYsthUcS89",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<b>Image Augmentation</b> If we consider example of dog image in given dataset and all dogs are facing left side then the model may not recognize all the dog facing right side after it its trained for good accuracy which may lead to overfiitting. To reduce over fitting and to improve accuracy we use image augmentation which genrates images from given datasets and produces more images with different angle and flips for better results and also we resize the images into smaller size  as the layers in the intial stages perfoms same pattern detection irrelevant of image size, therfore by reducing the size we can train our model faster and using less computation and then apply the result from this layers to images with large size"
      ]
    },
    {
      "metadata": {
        "id": "z5YsLOYD9aWe",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "generator = ImageDataGenerator(width_shift_range=5. / 32,\n",
        "                               height_shift_range=5. / 32,\n",
        "                               horizontal_flip=True)\n",
        "\n",
        "generator.fit(trainX, seed=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bp7tnj3ecmrC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Experiment\n",
        "\n",
        "<div align=\"justify\">**Objective:** Create a model using <b><i>Sparsenet</i></b> architecture and train the model on <i><b>CIFAR100</b></i> dataset</div>\n",
        "\n",
        "<div align=\"justify\">In this model i use densnet architecture for our model and replace the concatenation part with exponential $2^N$ function keeping all the hyper parameters and model architecture same. As i have already explained why sparse net exponential term has advantage over denset and resnet i would like to present the facts in below experiment\n",
        "  \n",
        " I use <i><b>Stochastic Gradient Descent (SGD)</b></i> optimizer. I have set the learning rate as 0.1 and i have also use momentum parameter which helps accelerate SGD in right direction. During the experiment i used  3 methods to set learning rate during training  \n",
        " 1. <b>Time-Based Learning Rate Schedule</b> in which we decay the learning rate by certain value at each epoch let say $$\\\\text{decay} = \\text{learning rate} /\\text{ no of epoch}$$\n",
        " <br>\n",
        " 2.<b> Drop-Based Learning Rate Schedule</b> in which we reduce learning rate after certain epochs for example in densnet paper the initial lr=0.1 and reduce to 0.01 when epoch reaches 50% and 0.001 when epoch reaches to 75% \n",
        " \n",
        "Both the above method are form of <i><b>Adaptive Learning Rate</b></i> in which we reduce learning rate linearly but in both cases what should be the base learning rate since if learning rate is to low it lead to slow convergence and if it is high it may lead to divergence and we may need to train model with diifferent learning rate to find the optimal result\n",
        "</br></br>\n",
        "\n",
        "### Experiment Result\n",
        "\n",
        "### val_acc =  69.620 for 250 Epochs using SGD Optimizer along with Learning Rate scheduler\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "pK0T9slc9wG_",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "5dcb1bb4-3969-40d2-f4dd-35f97ad46a94",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1530011289073,
          "user_tz": -330,
          "elapsed": 1011,
          "user": {
            "displayName": "Rahul Jain",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "117177615195402907037"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#optimizer\n",
        "optimizer = SGD(lr=0.0,decay=0.0, momentum=0.9,nesterov=True)  # Using SGD with Learning Rate Scheduler\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=[\"accuracy\"])\n",
        "print(\"Finished compiling\")\n",
        "print(\"Building model...\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Finished compiling\n",
            "Building model...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DC0se0HB3uhd",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Learning Rate Schedule\n",
        "def step_decay(epoch):\n",
        "  lr = 0.1\n",
        "  if epoch == int(0.5 * nb_epoch_32):\n",
        "    lr = np.float32(learning_rate / 10.)\n",
        "  if epoch == int(0.75 * nb_epoch_32):\n",
        "    lr = np.float32(learning_rate / 100.)\n",
        "  return lr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wXhU1dwtab7n",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "lrate = LearningRateScheduler(step_decay)\n",
        "model_checkpoint = ModelCheckpoint(weights_file, monitor=\"val_acc\", save_best_only=True, save_weights_only=True, verbose=1)\n",
        "\n",
        "callbacks = [lrate, model_checkpoint]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sg2dgaQM2OXD",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 1142
        },
        "outputId": "fd064b5f-1e45-495c-9fa0-e28f85a7b72f",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1530014785922,
          "user_tz": -330,
          "elapsed": 3423308,
          "user": {
            "displayName": "Rahul Jain",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "117177615195402907037"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#size 24\n",
        "#the learning rate is kept constant at 0.1\n",
        "trainX_24 = [imresize(image, (24, 24, 3)) for image in trainX]\n",
        "trainX_24 = np.array(trainX)\n",
        "\n",
        "model.fit_generator(generator.flow(trainX_24, Y_train, batch_size=64),\n",
        "                    steps_per_epoch=len(trainX_24) // batch_size, epochs=nb_epoch_24,\n",
        "                    callbacks=callbacks,\n",
        "                    validation_data=(testX, Y_test),\n",
        "                    validation_steps=testX.shape[0] // batch_size, verbose=1)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/scipy/misc/pilutil.py:482: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if issubdtype(ts, int):\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/misc/pilutil.py:485: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
            "  elif issubdtype(type(size), float):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "781/781 [==============================] - 235s 301ms/step - loss: 3.9762 - acc: 0.0845 - val_loss: 3.6385 - val_acc: 0.1304\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.13040, saving model to SparseNet-40-24-CIFAR100.h5\n",
            "Epoch 2/15\n",
            " 85/781 [==>...........................] - ETA: 3:08 - loss: 3.5948 - acc: 0.1382"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "781/781 [==============================] - 225s 289ms/step - loss: 3.3474 - acc: 0.1872 - val_loss: 3.0341 - val_acc: 0.2516\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.13040 to 0.25160, saving model to SparseNet-40-24-CIFAR100.h5\n",
            "Epoch 3/15\n",
            "136/781 [====>.........................] - ETA: 2:54 - loss: 3.0103 - acc: 0.2448"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "781/781 [==============================] - 226s 289ms/step - loss: 2.8433 - acc: 0.2818 - val_loss: 2.6010 - val_acc: 0.3284\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.25160 to 0.32840, saving model to SparseNet-40-24-CIFAR100.h5\n",
            "Epoch 4/15\n",
            "149/781 [====>.........................] - ETA: 2:50 - loss: 2.5723 - acc: 0.3324"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "781/781 [==============================] - 226s 289ms/step - loss: 2.4661 - acc: 0.3591 - val_loss: 2.5589 - val_acc: 0.3571\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.32840 to 0.35710, saving model to SparseNet-40-24-CIFAR100.h5\n",
            "Epoch 5/15\n",
            "152/781 [====>.........................] - ETA: 2:48 - loss: 2.2504 - acc: 0.4055"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "781/781 [==============================] - 225s 288ms/step - loss: 2.2141 - acc: 0.4167 - val_loss: 2.2690 - val_acc: 0.4155\n",
            "\n",
            "Epoch 00005: val_acc improved from 0.35710 to 0.41550, saving model to SparseNet-40-24-CIFAR100.h5\n",
            "Epoch 6/15\n",
            "153/781 [====>.........................] - ETA: 2:49 - loss: 2.0702 - acc: 0.4454"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "781/781 [==============================] - 225s 289ms/step - loss: 2.0326 - acc: 0.4559 - val_loss: 1.9597 - val_acc: 0.4863\n",
            "\n",
            "Epoch 00006: val_acc improved from 0.41550 to 0.48630, saving model to SparseNet-40-24-CIFAR100.h5\n",
            "Epoch 7/15\n",
            "153/781 [====>.........................] - ETA: 2:49 - loss: 1.8771 - acc: 0.4952"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "781/781 [==============================] - 225s 289ms/step - loss: 1.8853 - acc: 0.4896 - val_loss: 1.9028 - val_acc: 0.4979\n",
            "\n",
            "Epoch 00007: val_acc improved from 0.48630 to 0.49790, saving model to SparseNet-40-24-CIFAR100.h5\n",
            "Epoch 8/15\n",
            "153/781 [====>.........................] - ETA: 2:49 - loss: 1.7641 - acc: 0.5221"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "781/781 [==============================] - 226s 289ms/step - loss: 1.7808 - acc: 0.5179 - val_loss: 1.9289 - val_acc: 0.4986\n",
            "\n",
            "Epoch 00008: val_acc improved from 0.49790 to 0.49860, saving model to SparseNet-40-24-CIFAR100.h5\n",
            "Epoch 9/15\n",
            "153/781 [====>.........................] - ETA: 2:49 - loss: 1.6582 - acc: 0.5460"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "781/781 [==============================] - 226s 289ms/step - loss: 1.6823 - acc: 0.5422 - val_loss: 1.8778 - val_acc: 0.5110\n",
            "\n",
            "Epoch 00009: val_acc improved from 0.49860 to 0.51100, saving model to SparseNet-40-24-CIFAR100.h5\n",
            "Epoch 10/15\n",
            "153/781 [====>.........................] - ETA: 2:48 - loss: 1.5962 - acc: 0.5611"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "781/781 [==============================] - 226s 289ms/step - loss: 1.6131 - acc: 0.5627 - val_loss: 1.7382 - val_acc: 0.5412\n",
            "\n",
            "Epoch 00010: val_acc improved from 0.51100 to 0.54120, saving model to SparseNet-40-24-CIFAR100.h5\n",
            "Epoch 11/15\n",
            "153/781 [====>.........................] - ETA: 2:49 - loss: 1.5452 - acc: 0.5818"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "781/781 [==============================] - 225s 288ms/step - loss: 1.5502 - acc: 0.5778 - val_loss: 1.7186 - val_acc: 0.5468\n",
            "\n",
            "Epoch 00011: val_acc improved from 0.54120 to 0.54680, saving model to SparseNet-40-24-CIFAR100.h5\n",
            "Epoch 12/15\n",
            "153/781 [====>.........................] - ETA: 2:49 - loss: 1.4709 - acc: 0.6049"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "781/781 [==============================] - 225s 289ms/step - loss: 1.4947 - acc: 0.5920 - val_loss: 1.6453 - val_acc: 0.5647\n",
            "\n",
            "Epoch 00012: val_acc improved from 0.54680 to 0.56470, saving model to SparseNet-40-24-CIFAR100.h5\n",
            "Epoch 13/15\n",
            "153/781 [====>.........................] - ETA: 2:49 - loss: 1.4154 - acc: 0.6125"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "781/781 [==============================] - 226s 289ms/step - loss: 1.4426 - acc: 0.6070 - val_loss: 1.7564 - val_acc: 0.5595\n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.56470\n",
            "Epoch 14/15\n",
            "182/781 [=====>........................] - ETA: 2:40 - loss: 1.4037 - acc: 0.6126"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "781/781 [==============================] - 225s 289ms/step - loss: 1.3997 - acc: 0.6144 - val_loss: 1.6446 - val_acc: 0.5711\n",
            "\n",
            "Epoch 00014: val_acc improved from 0.56470 to 0.57110, saving model to SparseNet-40-24-CIFAR100.h5\n",
            "Epoch 15/15\n",
            "160/781 [=====>........................] - ETA: 2:46 - loss: 1.3232 - acc: 0.6368"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "781/781 [==============================] - 225s 288ms/step - loss: 1.3563 - acc: 0.6295 - val_loss: 1.7160 - val_acc: 0.5581\n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.57110\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fac9de6fcf8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "metadata": {
        "id": "CLzu8uZX2Llj",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 6259
        },
        "outputId": "a0b2cb39-4323-4fdb-9a0c-f7301240edf7"
      },
      "cell_type": "code",
      "source": [
        "lrate = LearningRateScheduler(step_decay)\n",
        "model_checkpoint = ModelCheckpoint(weights_file, monitor=\"val_acc\", save_best_only=True, save_weights_only=True, verbose=1)\n",
        "\n",
        "callbacks = [lrate, model_checkpoint]\n",
        "#size 32\n",
        "model.fit_generator(generator.flow(trainX, Y_train, batch_size=batch_size),\n",
        "                    steps_per_epoch=len(trainX) // batch_size, epochs=nb_epoch_32,\n",
        "                    callbacks=callbacks,\n",
        "                    validation_data=(testX, Y_test),\n",
        "                    validation_steps=testX.shape[0] // batch_size, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/135\n",
            "781/781 [==============================] - 225s 288ms/step - loss: 0.5249 - acc: 0.8623 - val_loss: 1.7369 - val_acc: 0.6602\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.66020, saving model to SparseNet-40-24-CIFAR100.h5\n",
            "Epoch 2/135\n",
            " 82/781 [==>...........................] - ETA: 3:07 - loss: 0.4533 - acc: 0.8843"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "781/781 [==============================] - 224s 287ms/step - loss: 0.4539 - acc: 0.8848 - val_loss: 1.6383 - val_acc: 0.6759\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.66020 to 0.67590, saving model to SparseNet-40-24-CIFAR100.h5\n",
            "Epoch 3/135\n",
            "135/781 [====>.........................] - ETA: 2:51 - loss: 0.4288 - acc: 0.8922"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "781/781 [==============================] - 223s 285ms/step - loss: 0.4318 - acc: 0.8915 - val_loss: 1.6917 - val_acc: 0.6704\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.67590\n",
            "Epoch 4/135\n",
            "177/781 [=====>........................] - ETA: 2:40 - loss: 0.4286 - acc: 0.8900"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "781/781 [==============================] - 223s 286ms/step - loss: 0.4173 - acc: 0.8959 - val_loss: 1.6014 - val_acc: 0.6774\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.67590 to 0.67740, saving model to SparseNet-40-24-CIFAR100.h5\n",
            "Epoch 5/135\n",
            "159/781 [=====>........................] - ETA: 2:46 - loss: 0.4141 - acc: 0.8979"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "781/781 [==============================] - 225s 288ms/step - loss: 0.4043 - acc: 0.9014 - val_loss: 1.6858 - val_acc: 0.6717\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.67740\n",
            "Epoch 6/135\n",
            "184/781 [======>.......................] - ETA: 2:39 - loss: 0.4076 - acc: 0.9028"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "781/781 [==============================] - 225s 288ms/step - loss: 0.3978 - acc: 0.9041 - val_loss: 1.5898 - val_acc: 0.6796\n",
            "\n",
            "Epoch 00006: val_acc improved from 0.67740 to 0.67960, saving model to SparseNet-40-24-CIFAR100.h5\n",
            "Epoch 7/135\n",
            "161/781 [=====>........................] - ETA: 2:45 - loss: 0.3839 - acc: 0.9117"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "781/781 [==============================] - 224s 287ms/step - loss: 0.3898 - acc: 0.9056 - val_loss: 1.5754 - val_acc: 0.6790\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.67960\n",
            "Epoch 8/135\n",
            "184/781 [======>.......................] - ETA: 2:40 - loss: 0.3934 - acc: 0.9046"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "781/781 [==============================] - 224s 287ms/step - loss: 0.3888 - acc: 0.9063 - val_loss: 1.6111 - val_acc: 0.6805\n",
            "\n",
            "Epoch 00008: val_acc improved from 0.67960 to 0.68050, saving model to SparseNet-40-24-CIFAR100.h5\n",
            "Epoch 9/135\n",
            "161/781 [=====>........................] - ETA: 2:45 - loss: 0.3891 - acc: 0.9032"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "781/781 [==============================] - 224s 287ms/step - loss: 0.3816 - acc: 0.9078 - val_loss: 1.5185 - val_acc: 0.6870\n",
            "\n",
            "Epoch 00009: val_acc improved from 0.68050 to 0.68700, saving model to SparseNet-40-24-CIFAR100.h5\n",
            "Epoch 10/135\n",
            "155/781 [====>.........................] - ETA: 2:47 - loss: 0.3740 - acc: 0.9071"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "781/781 [==============================] - 225s 287ms/step - loss: 0.3745 - acc: 0.9102 - val_loss: 1.5709 - val_acc: 0.6846\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.68700\n",
            "Epoch 11/135\n",
            "182/781 [=====>........................] - ETA: 2:40 - loss: 0.3786 - acc: 0.9076"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "781/781 [==============================] - 224s 287ms/step - loss: 0.3713 - acc: 0.9097 - val_loss: 1.5907 - val_acc: 0.6807\n",
            "\n",
            "Epoch 00011: val_acc did not improve from 0.68700\n",
            "Epoch 12/135\n",
            "190/781 [======>.......................] - ETA: 2:38 - loss: 0.3647 - acc: 0.9106"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "781/781 [==============================] - 224s 287ms/step - loss: 0.3666 - acc: 0.9127 - val_loss: 1.5407 - val_acc: 0.6872\n",
            "\n",
            "Epoch 00012: val_acc improved from 0.68700 to 0.68720, saving model to SparseNet-40-24-CIFAR100.h5\n",
            "Epoch 13/135\n",
            "162/781 [=====>........................] - ETA: 2:45 - loss: 0.3541 - acc: 0.9169"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "781/781 [==============================] - 224s 287ms/step - loss: 0.3645 - acc: 0.9135 - val_loss: 1.5730 - val_acc: 0.6864\n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.68720\n",
            "Epoch 14/135\n",
            "184/781 [======>.......................] - ETA: 2:39 - loss: 0.3597 - acc: 0.9171"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "781/781 [==============================] - 224s 287ms/step - loss: 0.3571 - acc: 0.9165 - val_loss: 1.5685 - val_acc: 0.6879\n",
            "\n",
            "Epoch 00014: val_acc improved from 0.68720 to 0.68790, saving model to SparseNet-40-24-CIFAR100.h5\n",
            "Epoch 15/135\n",
            "160/781 [=====>........................] - ETA: 2:45 - loss: 0.3736 - acc: 0.9078"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "781/781 [==============================] - 224s 286ms/step - loss: 0.3610 - acc: 0.9142 - val_loss: 1.5544 - val_acc: 0.6833\n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.68790\n",
            "Epoch 16/135\n",
            "183/781 [======>.......................] - ETA: 2:39 - loss: 0.3671 - acc: 0.9095"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "781/781 [==============================] - 224s 287ms/step - loss: 0.3545 - acc: 0.9152 - val_loss: 1.5700 - val_acc: 0.6831\n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.68790\n",
            "Epoch 17/135\n",
            "190/781 [======>.......................] - ETA: 2:37 - loss: 0.3571 - acc: 0.9177"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "781/781 [==============================] - 224s 286ms/step - loss: 0.3528 - acc: 0.9172 - val_loss: 1.5390 - val_acc: 0.6866\n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.68790\n",
            "Epoch 18/135\n",
            "192/781 [======>.......................] - ETA: 2:36 - loss: 0.3542 - acc: 0.9152"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "781/781 [==============================] - 223s 286ms/step - loss: 0.3525 - acc: 0.9163 - val_loss: 1.5767 - val_acc: 0.6845\n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.68790\n",
            "Epoch 19/135\n",
            "193/781 [======>.......................] - ETA: 2:36 - loss: 0.3487 - acc: 0.9165"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "781/781 [==============================] - 223s 286ms/step - loss: 0.3463 - acc: 0.9179 - val_loss: 1.5391 - val_acc: 0.6883\n",
            "\n",
            "Epoch 00019: val_acc improved from 0.68790 to 0.68830, saving model to SparseNet-40-24-CIFAR100.h5\n",
            "Epoch 20/135\n",
            "163/781 [=====>........................] - ETA: 2:45 - loss: 0.3465 - acc: 0.9191"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "781/781 [==============================] - 224s 286ms/step - loss: 0.3464 - acc: 0.9183 - val_loss: 1.5831 - val_acc: 0.6837\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.68830\n",
            "Epoch 21/135\n",
            "184/781 [======>.......................] - ETA: 2:39 - loss: 0.3478 - acc: 0.9167"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "781/781 [==============================] - 224s 287ms/step - loss: 0.3457 - acc: 0.9193 - val_loss: 1.5392 - val_acc: 0.6957\n",
            "\n",
            "Epoch 00021: val_acc improved from 0.68830 to 0.69570, saving model to SparseNet-40-24-CIFAR100.h5\n",
            "Epoch 22/135\n",
            "160/781 [=====>........................] - ETA: 2:45 - loss: 0.3335 - acc: 0.9231"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "781/781 [==============================] - 224s 286ms/step - loss: 0.3412 - acc: 0.9196 - val_loss: 1.5471 - val_acc: 0.6881\n",
            "\n",
            "Epoch 00022: val_acc did not improve from 0.69570\n",
            "Epoch 23/135\n",
            "183/781 [======>.......................] - ETA: 2:39 - loss: 0.3359 - acc: 0.9203"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "781/781 [==============================] - 224s 287ms/step - loss: 0.3402 - acc: 0.9202 - val_loss: 1.5230 - val_acc: 0.6912\n",
            "\n",
            "Epoch 00023: val_acc did not improve from 0.69570\n",
            "Epoch 24/135\n",
            "190/781 [======>.......................] - ETA: 2:38 - loss: 0.3333 - acc: 0.9219"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "781/781 [==============================] - 225s 287ms/step - loss: 0.3381 - acc: 0.9198 - val_loss: 1.6205 - val_acc: 0.6800\n",
            "\n",
            "Epoch 00024: val_acc did not improve from 0.69570\n",
            "Epoch 25/135\n",
            "192/781 [======>.......................] - ETA: 2:37 - loss: 0.3379 - acc: 0.9199"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "781/781 [==============================] - 224s 287ms/step - loss: 0.3312 - acc: 0.9233 - val_loss: 1.5438 - val_acc: 0.6907\n",
            "\n",
            "Epoch 00025: val_acc did not improve from 0.69570\n",
            "Epoch 26/135\n",
            "193/781 [======>.......................] - ETA: 2:37 - loss: 0.3245 - acc: 0.9241"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "781/781 [==============================] - 225s 288ms/step - loss: 0.3316 - acc: 0.9223 - val_loss: 1.6199 - val_acc: 0.6809\n",
            "\n",
            "Epoch 00026: val_acc did not improve from 0.69570\n",
            "Epoch 27/135\n",
            "193/781 [======>.......................] - ETA: 2:37 - loss: 0.3279 - acc: 0.9244"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "781/781 [==============================] - 225s 288ms/step - loss: 0.3299 - acc: 0.9232 - val_loss: 1.6179 - val_acc: 0.6839\n",
            "\n",
            "Epoch 00027: val_acc did not improve from 0.69570\n",
            "Epoch 28/135\n",
            "193/781 [======>.......................] - ETA: 2:38 - loss: 0.3315 - acc: 0.9230"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "781/781 [==============================] - 225s 288ms/step - loss: 0.3353 - acc: 0.9216 - val_loss: 1.6385 - val_acc: 0.6833\n",
            "\n",
            "Epoch 00028: val_acc did not improve from 0.69570\n",
            "Epoch 29/135\n",
            "193/781 [======>.......................] - ETA: 2:37 - loss: 0.3334 - acc: 0.9204"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "781/781 [==============================] - 224s 287ms/step - loss: 0.3324 - acc: 0.9215 - val_loss: 1.6230 - val_acc: 0.6871\n",
            "\n",
            "Epoch 00029: val_acc did not improve from 0.69570\n",
            "Epoch 30/135\n",
            "193/781 [======>.......................] - ETA: 2:38 - loss: 0.3341 - acc: 0.9223"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "781/781 [==============================] - 225s 288ms/step - loss: 0.3246 - acc: 0.9240 - val_loss: 1.5751 - val_acc: 0.6859\n",
            "\n",
            "Epoch 00030: val_acc did not improve from 0.69570\n",
            "Epoch 31/135\n",
            "193/781 [======>.......................] - ETA: 2:38 - loss: 0.3273 - acc: 0.9249"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "781/781 [==============================] - 225s 288ms/step - loss: 0.3284 - acc: 0.9238 - val_loss: 1.5561 - val_acc: 0.6876\n",
            "\n",
            "Epoch 00031: val_acc did not improve from 0.69570\n",
            "Epoch 32/135\n",
            "193/781 [======>.......................] - ETA: 2:36 - loss: 0.3200 - acc: 0.9239"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "781/781 [==============================] - 224s 287ms/step - loss: 0.3252 - acc: 0.9236 - val_loss: 1.5715 - val_acc: 0.6880\n",
            "\n",
            "Epoch 00032: val_acc did not improve from 0.69570\n",
            "Epoch 33/135\n",
            "193/781 [======>.......................] - ETA: 2:37 - loss: 0.3183 - acc: 0.9255"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "781/781 [==============================] - 225s 288ms/step - loss: 0.3244 - acc: 0.9235 - val_loss: 1.5743 - val_acc: 0.6902\n",
            "\n",
            "Epoch 00033: val_acc did not improve from 0.69570\n",
            "Epoch 34/135\n",
            "193/781 [======>.......................] - ETA: 2:37 - loss: 0.3365 - acc: 0.9216"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "781/781 [==============================] - 225s 288ms/step - loss: 0.3239 - acc: 0.9248 - val_loss: 1.5990 - val_acc: 0.6872\n",
            "\n",
            "Epoch 00034: val_acc did not improve from 0.69570\n",
            "Epoch 35/135\n",
            "193/781 [======>.......................] - ETA: 2:38 - loss: 0.3268 - acc: 0.9230"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "781/781 [==============================] - 225s 288ms/step - loss: 0.3210 - acc: 0.9254 - val_loss: 1.6480 - val_acc: 0.6879\n",
            "\n",
            "Epoch 00035: val_acc did not improve from 0.69570\n",
            "Epoch 36/135\n",
            "193/781 [======>.......................] - ETA: 2:37 - loss: 0.3120 - acc: 0.9297"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "781/781 [==============================] - 225s 288ms/step - loss: 0.3174 - acc: 0.9261 - val_loss: 1.6237 - val_acc: 0.6851\n",
            "\n",
            "Epoch 00036: val_acc did not improve from 0.69570\n",
            "Epoch 37/135\n",
            "193/781 [======>.......................] - ETA: 2:37 - loss: 0.3239 - acc: 0.9223"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "781/781 [==============================] - 224s 287ms/step - loss: 0.3186 - acc: 0.9264 - val_loss: 1.5803 - val_acc: 0.6905\n",
            "\n",
            "Epoch 00037: val_acc did not improve from 0.69570\n",
            "Epoch 38/135\n",
            "193/781 [======>.......................] - ETA: 2:36 - loss: 0.3203 - acc: 0.9252"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "781/781 [==============================] - 224s 287ms/step - loss: 0.3120 - acc: 0.9279 - val_loss: 1.5760 - val_acc: 0.6881\n",
            "\n",
            "Epoch 00038: val_acc did not improve from 0.69570\n",
            "Epoch 39/135\n",
            "193/781 [======>.......................] - ETA: 2:36 - loss: 0.3153 - acc: 0.9267"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "781/781 [==============================] - 224s 286ms/step - loss: 0.3178 - acc: 0.9244 - val_loss: 1.5437 - val_acc: 0.6951\n",
            "\n",
            "Epoch 00039: val_acc did not improve from 0.69570\n",
            "Epoch 40/135\n",
            "193/781 [======>.......................] - ETA: 2:37 - loss: 0.3104 - acc: 0.9286"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "781/781 [==============================] - 224s 287ms/step - loss: 0.3102 - acc: 0.9289 - val_loss: 1.5951 - val_acc: 0.6850\n",
            "\n",
            "Epoch 00040: val_acc did not improve from 0.69570\n",
            "Epoch 41/135\n",
            "193/781 [======>.......................] - ETA: 2:36 - loss: 0.3190 - acc: 0.9263"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "781/781 [==============================] - 225s 288ms/step - loss: 0.3174 - acc: 0.9256 - val_loss: 1.6256 - val_acc: 0.6872\n",
            "\n",
            "Epoch 00041: val_acc did not improve from 0.69570\n",
            "Epoch 42/135\n",
            "193/781 [======>.......................] - ETA: 2:38 - loss: 0.3070 - acc: 0.9275"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "781/781 [==============================] - 225s 288ms/step - loss: 0.3133 - acc: 0.9258 - val_loss: 1.6586 - val_acc: 0.6832\n",
            "\n",
            "Epoch 00042: val_acc did not improve from 0.69570\n",
            "Epoch 43/135\n",
            "193/781 [======>.......................] - ETA: 2:38 - loss: 0.3099 - acc: 0.9283"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "781/781 [==============================] - 225s 288ms/step - loss: 0.3150 - acc: 0.9266 - val_loss: 1.6072 - val_acc: 0.6898\n",
            "\n",
            "Epoch 00043: val_acc did not improve from 0.69570\n",
            "Epoch 44/135\n",
            "193/781 [======>.......................] - ETA: 2:38 - loss: 0.3084 - acc: 0.9287"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "781/781 [==============================] - 225s 288ms/step - loss: 0.3056 - acc: 0.9293 - val_loss: 1.5966 - val_acc: 0.6855\n",
            "\n",
            "Epoch 00044: val_acc did not improve from 0.69570\n",
            "Epoch 45/135\n",
            "193/781 [======>.......................] - ETA: 2:37 - loss: 0.3075 - acc: 0.9295"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "781/781 [==============================] - 225s 288ms/step - loss: 0.3090 - acc: 0.9286 - val_loss: 1.6038 - val_acc: 0.6885\n",
            "\n",
            "Epoch 00045: val_acc did not improve from 0.69570\n",
            "Epoch 46/135\n",
            "193/781 [======>.......................] - ETA: 2:37 - loss: 0.3131 - acc: 0.9262"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "781/781 [==============================] - 225s 288ms/step - loss: 0.3081 - acc: 0.9288 - val_loss: 1.6413 - val_acc: 0.6827\n",
            "\n",
            "Epoch 00046: val_acc did not improve from 0.69570\n",
            "Epoch 47/135\n",
            "193/781 [======>.......................] - ETA: 2:37 - loss: 0.3036 - acc: 0.9309"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "781/781 [==============================] - 225s 288ms/step - loss: 0.3040 - acc: 0.9294 - val_loss: 1.6319 - val_acc: 0.6854\n",
            "\n",
            "Epoch 00047: val_acc did not improve from 0.69570\n",
            "Epoch 48/135\n",
            "193/781 [======>.......................] - ETA: 2:37 - loss: 0.3025 - acc: 0.9306"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "781/781 [==============================] - 225s 288ms/step - loss: 0.3060 - acc: 0.9282 - val_loss: 1.6050 - val_acc: 0.6880\n",
            "\n",
            "Epoch 00048: val_acc did not improve from 0.69570\n",
            "Epoch 49/135\n",
            "193/781 [======>.......................] - ETA: 2:37 - loss: 0.2977 - acc: 0.9301"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "781/781 [==============================] - 224s 287ms/step - loss: 0.3009 - acc: 0.9301 - val_loss: 1.5991 - val_acc: 0.6865\n",
            "\n",
            "Epoch 00049: val_acc did not improve from 0.69570\n",
            "Epoch 50/135\n",
            "193/781 [======>.......................] - ETA: 2:37 - loss: 0.3086 - acc: 0.9271"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "781/781 [==============================] - 224s 287ms/step - loss: 0.3058 - acc: 0.9286 - val_loss: 1.6054 - val_acc: 0.6906\n",
            "\n",
            "Epoch 00050: val_acc did not improve from 0.69570\n",
            "Epoch 51/135\n",
            "193/781 [======>.......................] - ETA: 2:37 - loss: 0.2982 - acc: 0.9301"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "781/781 [==============================] - 225s 288ms/step - loss: 0.2958 - acc: 0.9315 - val_loss: 1.5718 - val_acc: 0.6930\n",
            "\n",
            "Epoch 00051: val_acc did not improve from 0.69570\n",
            "Epoch 52/135\n",
            "193/781 [======>.......................] - ETA: 2:37 - loss: 0.2982 - acc: 0.9296"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "781/781 [==============================] - 225s 288ms/step - loss: 0.2986 - acc: 0.9299 - val_loss: 1.6192 - val_acc: 0.6846\n",
            "\n",
            "Epoch 00052: val_acc did not improve from 0.69570\n",
            "Epoch 53/135\n",
            "193/781 [======>.......................] - ETA: 2:37 - loss: 0.3009 - acc: 0.9284"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "781/781 [==============================] - 225s 288ms/step - loss: 0.3012 - acc: 0.9301 - val_loss: 1.6660 - val_acc: 0.6824\n",
            "\n",
            "Epoch 00053: val_acc did not improve from 0.69570\n",
            "Epoch 54/135\n",
            "193/781 [======>.......................] - ETA: 2:37 - loss: 0.2957 - acc: 0.9313"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "781/781 [==============================] - 225s 288ms/step - loss: 0.2960 - acc: 0.9318 - val_loss: 1.5837 - val_acc: 0.6924\n",
            "\n",
            "Epoch 00054: val_acc did not improve from 0.69570\n",
            "Epoch 55/135\n",
            "193/781 [======>.......................] - ETA: 2:37 - loss: 0.3001 - acc: 0.9265"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "781/781 [==============================] - 225s 288ms/step - loss: 0.2940 - acc: 0.9321 - val_loss: 1.6320 - val_acc: 0.6847\n",
            "\n",
            "Epoch 00055: val_acc did not improve from 0.69570\n",
            "Epoch 56/135\n",
            "193/781 [======>.......................] - ETA: 2:38 - loss: 0.2929 - acc: 0.9332"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "781/781 [==============================] - 225s 288ms/step - loss: 0.2965 - acc: 0.9315 - val_loss: 1.6438 - val_acc: 0.6889\n",
            "\n",
            "Epoch 00056: val_acc did not improve from 0.69570\n",
            "Epoch 57/135\n",
            "193/781 [======>.......................] - ETA: 2:37 - loss: 0.2973 - acc: 0.9301"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "781/781 [==============================] - 225s 288ms/step - loss: 0.2940 - acc: 0.9318 - val_loss: 1.5823 - val_acc: 0.6913\n",
            "\n",
            "Epoch 00057: val_acc did not improve from 0.69570\n",
            "Epoch 58/135\n",
            "193/781 [======>.......................] - ETA: 2:37 - loss: 0.2917 - acc: 0.9334"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "781/781 [==============================] - 225s 288ms/step - loss: 0.2905 - acc: 0.9328 - val_loss: 1.5950 - val_acc: 0.6931\n",
            "\n",
            "Epoch 00058: val_acc did not improve from 0.69570\n",
            "Epoch 59/135\n",
            "193/781 [======>.......................] - ETA: 2:37 - loss: 0.2940 - acc: 0.9318"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "781/781 [==============================] - 225s 288ms/step - loss: 0.2928 - acc: 0.9334 - val_loss: 1.6302 - val_acc: 0.6864\n",
            "\n",
            "Epoch 00059: val_acc did not improve from 0.69570\n",
            "Epoch 60/135\n",
            "193/781 [======>.......................] - ETA: 2:37 - loss: 0.2976 - acc: 0.9295"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "781/781 [==============================] - 224s 287ms/step - loss: 0.2952 - acc: 0.9316 - val_loss: 1.6120 - val_acc: 0.6871\n",
            "\n",
            "Epoch 00060: val_acc did not improve from 0.69570\n",
            "Epoch 61/135\n",
            "193/781 [======>.......................] - ETA: 2:37 - loss: 0.2999 - acc: 0.9290"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "781/781 [==============================] - 224s 287ms/step - loss: 0.2986 - acc: 0.9296 - val_loss: 1.5928 - val_acc: 0.6887\n",
            "\n",
            "Epoch 00061: val_acc did not improve from 0.69570\n",
            "Epoch 62/135\n",
            "193/781 [======>.......................] - ETA: 2:37 - loss: 0.2901 - acc: 0.9330"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "781/781 [==============================] - 225s 288ms/step - loss: 0.2885 - acc: 0.9326 - val_loss: 1.6007 - val_acc: 0.6901\n",
            "\n",
            "Epoch 00062: val_acc did not improve from 0.69570\n",
            "Epoch 63/135\n",
            "193/781 [======>.......................] - ETA: 2:37 - loss: 0.2916 - acc: 0.9330"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "781/781 [==============================] - 225s 288ms/step - loss: 0.2899 - acc: 0.9338 - val_loss: 1.5811 - val_acc: 0.6956\n",
            "\n",
            "Epoch 00063: val_acc did not improve from 0.69570\n",
            "Epoch 64/135\n",
            "193/781 [======>.......................] - ETA: 2:37 - loss: 0.2857 - acc: 0.9347"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "781/781 [==============================] - 225s 288ms/step - loss: 0.2893 - acc: 0.9313 - val_loss: 1.6453 - val_acc: 0.6820\n",
            "\n",
            "Epoch 00064: val_acc did not improve from 0.69570\n",
            "Epoch 65/135\n",
            "193/781 [======>.......................] - ETA: 2:38 - loss: 0.2912 - acc: 0.9332"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "781/781 [==============================] - 225s 288ms/step - loss: 0.2911 - acc: 0.9330 - val_loss: 1.6555 - val_acc: 0.6859\n",
            "\n",
            "Epoch 00065: val_acc did not improve from 0.69570\n",
            "Epoch 66/135\n",
            "193/781 [======>.......................] - ETA: 2:38 - loss: 0.2810 - acc: 0.9358"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "781/781 [==============================] - 225s 289ms/step - loss: 0.2836 - acc: 0.9343 - val_loss: 1.6792 - val_acc: 0.6842\n",
            "\n",
            "Epoch 00066: val_acc did not improve from 0.69570\n",
            "Epoch 67/135\n",
            "193/781 [======>.......................] - ETA: 2:38 - loss: 0.2888 - acc: 0.9301"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "781/781 [==============================] - 225s 289ms/step - loss: 0.2894 - acc: 0.9318 - val_loss: 1.7314 - val_acc: 0.6771\n",
            "\n",
            "Epoch 00067: val_acc did not improve from 0.69570\n",
            "Epoch 68/135\n",
            "193/781 [======>.......................] - ETA: 2:37 - loss: 0.2849 - acc: 0.9346"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "781/781 [==============================] - 224s 287ms/step - loss: 0.2877 - acc: 0.9330 - val_loss: 1.6024 - val_acc: 0.6940\n",
            "\n",
            "Epoch 00068: val_acc did not improve from 0.69570\n",
            "Epoch 69/135\n",
            "193/781 [======>.......................] - ETA: 2:37 - loss: 0.2818 - acc: 0.9357"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "781/781 [==============================] - 224s 287ms/step - loss: 0.2839 - acc: 0.9346 - val_loss: 1.5913 - val_acc: 0.6913\n",
            "\n",
            "Epoch 00069: val_acc did not improve from 0.69570\n",
            "Epoch 70/135\n",
            "193/781 [======>.......................] - ETA: 2:37 - loss: 0.2859 - acc: 0.9315"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "781/781 [==============================] - 225s 288ms/step - loss: 0.2855 - acc: 0.9331 - val_loss: 1.6358 - val_acc: 0.6900\n",
            "\n",
            "Epoch 00070: val_acc did not improve from 0.69570\n",
            "Epoch 71/135\n",
            "193/781 [======>.......................] - ETA: 2:37 - loss: 0.2816 - acc: 0.9362"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "781/781 [==============================] - 225s 288ms/step - loss: 0.2836 - acc: 0.9343 - val_loss: 1.6128 - val_acc: 0.6896\n",
            "\n",
            "Epoch 00071: val_acc did not improve from 0.69570\n",
            "Epoch 72/135\n",
            "193/781 [======>.......................] - ETA: 2:37 - loss: 0.2825 - acc: 0.9335"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "781/781 [==============================] - 224s 287ms/step - loss: 0.2817 - acc: 0.9343 - val_loss: 1.6058 - val_acc: 0.6945\n",
            "\n",
            "Epoch 00072: val_acc did not improve from 0.69570\n",
            "Epoch 73/135\n",
            "193/781 [======>.......................] - ETA: 2:36 - loss: 0.2870 - acc: 0.9305"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "781/781 [==============================] - 224s 287ms/step - loss: 0.2778 - acc: 0.9340 - val_loss: 1.6089 - val_acc: 0.6949\n",
            "\n",
            "Epoch 00073: val_acc did not improve from 0.69570\n",
            "Epoch 74/135\n",
            "193/781 [======>.......................] - ETA: 2:38 - loss: 0.2838 - acc: 0.9334"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "781/781 [==============================] - 225s 288ms/step - loss: 0.2821 - acc: 0.9345 - val_loss: 1.7105 - val_acc: 0.6778\n",
            "\n",
            "Epoch 00074: val_acc did not improve from 0.69570\n",
            "Epoch 75/135\n",
            "193/781 [======>.......................] - ETA: 2:38 - loss: 0.2782 - acc: 0.9351"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "781/781 [==============================] - 225s 289ms/step - loss: 0.2780 - acc: 0.9346 - val_loss: 1.6890 - val_acc: 0.6834\n",
            "\n",
            "Epoch 00075: val_acc did not improve from 0.69570\n",
            "Epoch 76/135\n",
            "193/781 [======>.......................] - ETA: 2:38 - loss: 0.2750 - acc: 0.9366"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "781/781 [==============================] - 226s 289ms/step - loss: 0.2783 - acc: 0.9341 - val_loss: 1.5654 - val_acc: 0.6962\n",
            "\n",
            "Epoch 00076: val_acc improved from 0.69570 to 0.69620, saving model to SparseNet-40-24-CIFAR100.h5\n",
            "Epoch 77/135\n",
            "163/781 [=====>........................] - ETA: 2:46 - loss: 0.2917 - acc: 0.9278"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "781/781 [==============================] - 225s 289ms/step - loss: 0.2815 - acc: 0.9336 - val_loss: 1.6173 - val_acc: 0.6906\n",
            "\n",
            "Epoch 00077: val_acc did not improve from 0.69620\n",
            "Epoch 78/135\n",
            "184/781 [======>.......................] - ETA: 2:40 - loss: 0.2717 - acc: 0.9373"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "781/781 [==============================] - 225s 289ms/step - loss: 0.2787 - acc: 0.9349 - val_loss: 1.6284 - val_acc: 0.6894\n",
            "\n",
            "Epoch 00078: val_acc did not improve from 0.69620\n",
            "Epoch 79/135\n",
            "191/781 [======>.......................] - ETA: 2:39 - loss: 0.2763 - acc: 0.9351"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "781/781 [==============================] - 225s 288ms/step - loss: 0.2771 - acc: 0.9343 - val_loss: 1.7604 - val_acc: 0.6830\n",
            "\n",
            "Epoch 00079: val_acc did not improve from 0.69620\n",
            "Epoch 80/135\n",
            "193/781 [======>.......................] - ETA: 2:37 - loss: 0.2695 - acc: 0.9377"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "781/781 [==============================] - 225s 288ms/step - loss: 0.2742 - acc: 0.9365 - val_loss: 1.6753 - val_acc: 0.6906\n",
            "\n",
            "Epoch 00080: val_acc did not improve from 0.69620\n",
            "Epoch 81/135\n",
            "193/781 [======>.......................] - ETA: 2:37 - loss: 0.2681 - acc: 0.9405"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "781/781 [==============================] - 225s 288ms/step - loss: 0.2717 - acc: 0.9370 - val_loss: 1.6399 - val_acc: 0.6896\n",
            "\n",
            "Epoch 00081: val_acc did not improve from 0.69620\n",
            "Epoch 82/135\n",
            "193/781 [======>.......................] - ETA: 2:37 - loss: 0.2823 - acc: 0.9325"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "781/781 [==============================] - 224s 287ms/step - loss: 0.2757 - acc: 0.9367 - val_loss: 1.6790 - val_acc: 0.6841\n",
            "\n",
            "Epoch 00082: val_acc did not improve from 0.69620\n",
            "Epoch 83/135\n",
            "193/781 [======>.......................] - ETA: 2:37 - loss: 0.2813 - acc: 0.9320"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "781/781 [==============================] - 224s 287ms/step - loss: 0.2740 - acc: 0.9367 - val_loss: 1.6449 - val_acc: 0.6875\n",
            "\n",
            "Epoch 00083: val_acc did not improve from 0.69620\n",
            "Epoch 84/135\n",
            "193/781 [======>.......................] - ETA: 2:37 - loss: 0.2711 - acc: 0.9395"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "781/781 [==============================] - 225s 288ms/step - loss: 0.2753 - acc: 0.9366 - val_loss: 1.6252 - val_acc: 0.6889\n",
            "\n",
            "Epoch 00084: val_acc did not improve from 0.69620\n",
            "Epoch 85/135\n",
            "193/781 [======>.......................] - ETA: 2:37 - loss: 0.2777 - acc: 0.9350"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "781/781 [==============================] - 225s 288ms/step - loss: 0.2730 - acc: 0.9366 - val_loss: 1.6539 - val_acc: 0.6887\n",
            "\n",
            "Epoch 00085: val_acc did not improve from 0.69620\n",
            "Epoch 86/135\n",
            "193/781 [======>.......................] - ETA: 2:37 - loss: 0.2722 - acc: 0.9360"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "781/781 [==============================] - 225s 288ms/step - loss: 0.2740 - acc: 0.9359 - val_loss: 1.6470 - val_acc: 0.6841\n",
            "\n",
            "Epoch 00086: val_acc did not improve from 0.69620\n",
            "Epoch 87/135\n",
            "193/781 [======>.......................] - ETA: 2:37 - loss: 0.2785 - acc: 0.9352"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "781/781 [==============================] - 225s 288ms/step - loss: 0.2725 - acc: 0.9361 - val_loss: 1.6403 - val_acc: 0.6949\n",
            "\n",
            "Epoch 00087: val_acc did not improve from 0.69620\n",
            "Epoch 88/135\n",
            "193/781 [======>.......................] - ETA: 2:38 - loss: 0.2759 - acc: 0.9354"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "781/781 [==============================] - 225s 289ms/step - loss: 0.2739 - acc: 0.9353 - val_loss: 1.6561 - val_acc: 0.6846\n",
            "\n",
            "Epoch 00088: val_acc did not improve from 0.69620\n",
            "Epoch 89/135\n",
            "193/781 [======>.......................] - ETA: 2:37 - loss: 0.2720 - acc: 0.9354"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "781/781 [==============================] - 225s 289ms/step - loss: 0.2708 - acc: 0.9363 - val_loss: 1.6382 - val_acc: 0.6911\n",
            "\n",
            "Epoch 00089: val_acc did not improve from 0.69620\n",
            "Epoch 90/135\n",
            "193/781 [======>.......................] - ETA: 2:38 - loss: 0.2731 - acc: 0.9374"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "781/781 [==============================] - 225s 289ms/step - loss: 0.2719 - acc: 0.9368 - val_loss: 1.7166 - val_acc: 0.6805\n",
            "\n",
            "Epoch 00090: val_acc did not improve from 0.69620\n",
            "Epoch 91/135\n",
            "193/781 [======>.......................] - ETA: 2:37 - loss: 0.2785 - acc: 0.9311"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "781/781 [==============================] - 225s 289ms/step - loss: 0.2711 - acc: 0.9355 - val_loss: 1.6865 - val_acc: 0.6883\n",
            "\n",
            "Epoch 00091: val_acc did not improve from 0.69620\n",
            "Epoch 92/135\n",
            "114/781 [===>..........................] - ETA: 2:59 - loss: 0.2680 - acc: 0.9360"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}