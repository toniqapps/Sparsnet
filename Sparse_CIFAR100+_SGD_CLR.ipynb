{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sparse_CIFAR100+_SGD_CLR.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "zmTl2xZ7Aom6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##SparseNet models for Keras.\n",
        "### Reference\n",
        "- [Sparsely Connected Convolutional Networks](https://arxiv.org/abs/1801.05895)\n",
        "- [Github](https://github.com/lyken17/sparsenet)\n",
        "- [Cyclical Learning Rates for Training Neural Networks\n",
        "](https://arxiv.org/abs/1506.01186)"
      ]
    },
    {
      "metadata": {
        "id": "s4aD2y3ZJS3N",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Skip Connections:\n",
        "---\n",
        "<div align=\"justify\">Predicting detail information of complicated visual scene may require understanding it at multiple levels abstraction, from edges and textures to object categories. As we all know convolution neural network learns increasingly abstract visual representations when going to deeper layers. But training such  deep networks requires back-propogating a signal through all the layers of the given networks which results in loss at the end of network to be noisier due to deeper layers and it becomes worst has we go deep and we also need to store and maintain a feature computed early that network need to reuse, to overcome this we use skip connections which connect multiple outputs from different layer to $l_{th}$ layer which can provide a pathway for assembling  feature that  combines many level of abstraction</div>"
      ]
    },
    {
      "metadata": {
        "id": "_8SXw_mhJYYf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Optimizer:\n",
        "___\n",
        "\n",
        "The choice of optimization algorithm indicates how fast  and optimum is your model.  To train a model we need to reduce the loss which is a functon of weight and bias. With the help of back propogation we back propogate the current error in the previous layer and modify the weights and bias such a way that the error is reduced. To modify weights we use optimization algorithm.\n",
        "\n",
        "Optimization function is usually use to calculate gradiant i.e. the partial derivative of loss function with respect to weights and weghts are modified in opposite direction of calculate gradient. This i cycle is repeated until we reach minima of loss function\n",
        "\n",
        "To reduce the loss and modify the weights we use hyper parameter learning rate which can be tuned to get optimal results. Choosing a proper learning rate is difficult. If the learning rate is too low it result in slow convergence and may lead to vanishing gradient descent  and if it is to high it may diverge from the minmal and may leading exploding gradient problem\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "QEsCZ7GpDBxT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Why Sparsenet : \n",
        "___\n",
        "<div align=\"justify\">SparseNet is a variant of DenseNets or Resnet. In Densenet we  have a skip connection after every block which are concatenated to next layer and in resnet we do cumulative summation, but both have same problems i.e. as the depth increases, the number of features grows linearly. Later features may corrupt or wash out the information carried by earlier features maps as seen in resnet which result in saturation of resnet performance, in contrast densnet preserves the original format of previous layers due to concatenation, this factor contribute to better parameter performance efficiency over resnet but due to concatenation no of parameters grows at the rate of $0(N^2)$ due to which portion of network is devoted to process previously seen feature map and hence are not able to exploit all the parameters fully and this pitfall are due to the linear growth of feature maps in both densnet and resnet. To overcome this we would like to maintain the power of short gradient paths for training. By aggregating features only from layers with exponential offset the length of the shortest  gradient path between blocks with offset S is bounded  by  $O((c-1)log(S))$. Here, c is again the base of the exponent governing the sparse connection pattern. The total number of output to the $l^{th}$ block is $O(log(l))$ due to exponential offsets. Therefore total no of skip connections is</div>\n",
        "\n",
        "<center>$\\sum_{l=1}^N(log_cl) = O(NlogN)$</center>\n",
        "\n",
        "<center>N is the number of basic blocks (depth) of the network. </center>\n",
        "\n",
        "<center>The number of parameters are $O(N log N)$ and $O(N)$, respectively, for aggregation by concatenation and aggregation by summation</center>\n",
        "\n",
        "<div align=\"justify\">SparseNets have such skip connections only at depths of $2^N$. This allows model to be less memory intensive due to less parameters while still performing equivalent to densnet or better</div>\n",
        "\n",
        "<center>![alt text](https://lh3.googleusercontent.com/ar5begWFXAGXPVDeIORZB_iD4OrsAe6dR-yyfEjCNhR8fnt-LnnFcRDUrecj7era4845nS8iyolaWmN0GaTCo114I9WmTSTo0cTIGBQnVwzvJ9yrVa0Fm0TYUnxphcHbQC5pAoWe=w2400)</center>\n",
        "\n",
        "<center>** Densnet/Resnet**</center>\n",
        "\n",
        "<center>![alt text](https://lh3.googleusercontent.com/pJosUXvPpuMJu87oi0gb351VelsWpLkbcX6TXx5i1qh_QOaMEPgeJS-Ikg3Dilfue6qDNnfOblaOpc8BUJOzgY4yPE23QxOBttS268ojfYJajR7uBGg__cNisOUUIp0f-vNfx7zi=w2400)</center>\n",
        "\n",
        "<center>![alt text](https://lh3.googleusercontent.com/EnnfenqM7PICFgtcfxgVO0PWJOCbpFFCCq0DDSRhBoZU63ZgZUzqAsGWx0tSZCbULSNwqfBDEr41Q0enZUJXAUON3j2s30aqosQZrrsgBHWHjWJpB4Xo5bBlm-NvmBkkXJNOTRMp=w2400)</center>\n",
        "<center>** Sparsenet**</center>\n"
      ]
    },
    {
      "metadata": {
        "id": "r2G45wc6C9YS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Experiments\n",
        "\n",
        "<div align=\"justify\">We demonstrate the effectiveness of SparseNets over DenseNets, through image classification tasks on the CIFAR-100 datasets</div>\n",
        "\n",
        "We Implement our models in Keras\n",
        "\n",
        "**Datasets**:\n",
        "CIFAR both the CIFAR-10 and CIFAR-100 datasets have 50,000 training\n",
        "images and 10,000 testing images with size of 32 Ã— 32 pixels. CIFAR-10 \n",
        "and CIFAR-100  have 10 and 100 classes respectively. Our experiments\n",
        "use standard data augmentation"
      ]
    },
    {
      "metadata": {
        "id": "iYZh5OwzAkql",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import warnings\n",
        "from scipy.misc import imresize, toimage\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "from keras.models import Model\n",
        "from keras.layers.core import Dense, Dropout, Activation, Reshape\n",
        "from keras.layers.convolutional import Conv2D, Conv2DTranspose, UpSampling2D, SeparableConv2D\n",
        "from keras.layers import AveragePooling2D, MaxPooling2D\n",
        "from keras.layers import GlobalAveragePooling2D\n",
        "from keras.layers import Input\n",
        "from keras.layers.merge import concatenate\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.regularizers import l2\n",
        "from keras.utils.layer_utils import convert_all_kernels_in_model, convert_dense_weights_data_format\n",
        "from keras.utils.data_utils import get_file\n",
        "from keras.utils import np_utils\n",
        "\n",
        "from keras.engine.topology import get_source_inputs\n",
        "from keras.applications.imagenet_utils import _obtain_input_shape\n",
        "from keras.applications.imagenet_utils import decode_predictions\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "import keras.backend as K\n",
        "\n",
        "\n",
        "import os.path\n",
        "import sklearn.metrics as metrics\n",
        "from keras.datasets import cifar100\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.optimizers import SGD\n",
        "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "g2zzSoJ5DnHb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Load Datasets"
      ]
    },
    {
      "metadata": {
        "id": "mAYkKZ6v7Iwa",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#load data\n",
        "(trainX, trainY), (testX, testY) = cifar100.load_data()\n",
        "\n",
        "nb_classes = len(np.unique(trainY))\n",
        "nb_channels = trainX.shape[-1]\n",
        "\n",
        "trainX = trainX.astype('float32')\n",
        "testX = testX.astype('float32')\n",
        "\n",
        "cifar_mean = trainX.mean(axis=(0, 1, 2), keepdims=True)\n",
        "cifar_std = trainX.std(axis=(0, 1, 2), keepdims=True)\n",
        "\n",
        "trainX = (trainX - cifar_mean) / (cifar_std + 1e-8)\n",
        "testX = (testX - cifar_mean) / (cifar_std + 1e-8)\n",
        "\n",
        "Y_train = np_utils.to_categorical(trainY, nb_classes)\n",
        "Y_test = np_utils.to_categorical(testY, nb_classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eFh7Eyh4DHhl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Hyper Parameters\n"
      ]
    },
    {
      "metadata": {
        "id": "aAycUnen-ayf",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "dropout_rate = 0.0\n",
        "growth_rate=24\n",
        "compression = 0.5\n",
        "depth = 40\n",
        "bottleneck=False\n",
        "weight_decay=1e-4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0KqfSvzGD9lX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Batch Normalization** : Use to normalize output of previous activation layer\n",
        "\n",
        "**Relu** : Use to convert all the negative values to zero and positive values unchanged\n",
        "\n",
        "**Conv2D**: Apply kernel with receptive field size of 3x3 with bias zero and padding same to maintain output feature map same as input feature map\n",
        "\n",
        "**Dropout**: Reducing overfiitting by dropping few node randomly\n",
        "\n",
        "**Bottleneck Layer** : To improve computational efficiency we can introduce bottleneck layer 3x3 convolution layer to reduce the no of feature maps\n",
        "\n",
        "**Compression**:  We can also reduce feature maps in transition layer by introducing compression factor whose value lies between $0<\\theta\\le1$"
      ]
    },
    {
      "metadata": {
        "id": "T9pRh1iwD4E_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Exponential $2^N$\n",
        "We consider skip connections only at depths of $2^N$.  \n",
        "\n",
        "**input** : List of processed layers\n",
        "\n",
        "**returns** : layers which are exponential of $2^N$"
      ]
    },
    {
      "metadata": {
        "id": "JzItM4I2BRr4",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def _exponential_index_fetch(x_list):\n",
        "    count = len(x_list)\n",
        "    i = 1\n",
        "    inputs = []\n",
        "    while i <= count:\n",
        "      inputs.append(x_list[count - i])\n",
        "      i *= 2\n",
        "    return inputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GWaC0FbvEJzK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Convolution Layer with &amp; Wihtout Bottleneck Layer\n",
        "\n",
        "###Parameters:\n",
        "\n",
        "**Input**: previous layer output (ip), num of filter (nb_filter)\n",
        "\n",
        "**Output** : current layer output (x)\n",
        "\n",
        "**note**: Both bottleneck layer and dropout are conditional "
      ]
    },
    {
      "metadata": {
        "id": "5ulmYZkCBZjz",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def add_conv_block(ip, nb_filter):\n",
        "    \n",
        "  concat_axis = -1\n",
        "\n",
        "  x = BatchNormalization(axis=concat_axis, momentum=0.1, epsilon=1e-5)(ip)\n",
        "  x = Activation('relu')(x)\n",
        "\n",
        "  if bottleneck:\n",
        "    # Obtained from https://github.com/liuzhuang13/DenseNet/blob/master/densenet.lua\n",
        "    inter_channel = nb_filter * 4 \n",
        "\n",
        "    x = Conv2D(inter_channel, (1, 1), kernel_initializer='he_normal', padding='same', use_bias=False,\n",
        "               kernel_regularizer=l2(weight_decay))(x)\n",
        "    x = BatchNormalization(axis=concat_axis, epsilon=1e-5, momentum=0.1)(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "  x = Conv2D(nb_filter, (3, 3), kernel_initializer='he_normal', padding='same', use_bias=False)(x)\n",
        "  if dropout_rate > 0.0:\n",
        "    x = Dropout(dropout_rate)(x)\n",
        "\n",
        "  return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "koOi5COMFZ41",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Dense Block\n",
        "\n",
        "###Parameters:\n",
        "\n",
        "**Input**: \n",
        "\n",
        "1.   previous layer output (ip)\n",
        "2.   no of layers per dense block (nb_layers)\n",
        "3. rate at which no of filter grow (growth_rate), grow filter exponentially               (grow_nb_filters)\n",
        "\n",
        "**Output** : \n",
        "1. current layer output (x) \n",
        "2. no of filter (nb_filter)\n",
        "\n",
        "###Explanation\n",
        "\n",
        "In this function we create layers based on no of layers (nb_layers)  and concatenate  output layers from previous layers  which are $2^N$  exponential and also grow no of filter exponentially \n"
      ]
    },
    {
      "metadata": {
        "id": "gHOTlQmjBctr",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def add_dense_block(x, nb_layers, nb_filter):\n",
        "    \n",
        "    concat_axis = -1\n",
        "\n",
        "    x_list = [x]\n",
        "    channel_list = [nb_filter]\n",
        "\n",
        "    for i in range(nb_layers):\n",
        "      x = add_conv_block(x, growth_rate)\n",
        "      x_list.append(x)\n",
        "\n",
        "      fetch_outputs = _exponential_index_fetch(x_list)\n",
        "      x = concatenate(fetch_outputs, axis=concat_axis)\n",
        "\n",
        "      channel_list.append(growth_rate)\n",
        "\n",
        "    nb_filter = sum(_exponential_index_fetch(channel_list))\n",
        "\n",
        "    return x, nb_filter"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zSkYgQuLFfSy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Transition Layer\n",
        "\n",
        "\n",
        "###Parameters:\n",
        "\n",
        "**Input**: \n",
        "\n",
        "1.   previous layer output (ip)\n",
        "2. no of filters (nb_filter)\n",
        "3. reduction ratio of transition layer (compression)\n",
        "\n",
        "**Output** : \n",
        "1. current layer output (x) \n",
        "\n",
        "###Explanation\n",
        "\n",
        "Use to reduce the no of feature maps after each dense block. This is use to control no of parameters that flow to next dense block for better computancy. Using compression we control the reduction ratio of transition layer"
      ]
    },
    {
      "metadata": {
        "id": "o1LKpdeQBg9G",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def add_transition_block(ip, nb_filter):\n",
        "  concat_axis = -1\n",
        "  x = BatchNormalization(axis=concat_axis, epsilon=1e-5, momentum=0.1)(ip)\n",
        "  x = Activation('relu')(x)\n",
        "  x = Conv2D(int(nb_filter * compression), (1, 1), kernel_initializer='he_normal', padding='same', use_bias=False,\n",
        "             kernel_regularizer=l2(weight_decay))(x)\n",
        "  x = AveragePooling2D((2, 2))(x)\n",
        "\n",
        "  return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "n8zfwIBgFsPP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Create Sparsenet\n",
        "\n",
        "\n",
        "###Parameters:\n",
        "\n",
        "**Input**: \n",
        "\n",
        "1.   Image Input (img_input)\n",
        "2.   total no of layer in the given sparsenet model (depth)\n",
        "4. growth rate\n",
        "\n",
        "**Output** : \n",
        "1. model output (x) \n",
        "\n",
        "###Explanation\n",
        "\n",
        "Based on given depth we divide total no layers into layers in dense block, layer in transition layer and layer in bottleneck layer using the below formula we divide the layers into the given blocks and layers\n",
        "<center>$Depth = 3N+4$</center>\n",
        "**note**: by default we consider 3 dense block and last dense block doesn't have transition layer\n",
        "\n",
        "We convert given no of dense block in our case 3 into a list and also growth rate into list remember both lost should be of dense block size\n",
        "\n",
        "We assign nb_filter = growth rate for firstlayer which is a convolution layer of kerner receptive field size of 3x3 following this layers is our 3 dense block and 2 transition layer and bottleneck layers if true and finall y the output layer"
      ]
    },
    {
      "metadata": {
        "id": "Bgj-OFe3BlUD",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def create_dense_net(img_input):\n",
        "   \n",
        "    global compression\n",
        "    #channel_last\n",
        "    concat_axis = -1\n",
        "    #no of dense_block\n",
        "    nb_dense_block=3\n",
        "\n",
        "    # layers in each dense block\n",
        "    assert (depth - 4) % nb_dense_block == 0, 'Depth must be 3 N + 4'\n",
        "    count = int((depth - 4) / nb_dense_block)\n",
        "\n",
        "    if bottleneck:\n",
        "      count = count // 2\n",
        "    else:\n",
        "      compression = 1\n",
        "\n",
        "    #convert int list\n",
        "    nb_layers = [count for _ in range(nb_dense_block)]\n",
        "    final_nb_layer = count\n",
        "\n",
        "    # compute initial nb_filter\n",
        "    nb_filter = 2 * growth_rate\n",
        "\n",
        "    # Initial convolution\n",
        "    x = Conv2D(nb_filter, (3,3), kernel_initializer='he_normal', padding='same', use_bias=False, kernel_regularizer=l2(weight_decay))(img_input)\n",
        "\n",
        "    # Add dense blocks\n",
        "    for i in range(nb_dense_block - 1):\n",
        "        # add dense block\n",
        "        x, nb_filter = add_dense_block(x, nb_layers[i], nb_filter)\n",
        "        # add transition_block\n",
        "        x = add_transition_block(x, nb_filter)\n",
        "        nb_filter = int(nb_filter * compression)\n",
        "\n",
        "    # The last dense_block does not have a transition_block\n",
        "    x, nb_filter = add_dense_block(x, final_nb_layer, nb_filter)\n",
        "\n",
        "    x = BatchNormalization(axis=concat_axis, epsilon=1e-5, momentum=0.1)(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "\n",
        "    x = Dense(nb_classes, activation='softmax')(x)\n",
        "\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Vpm513P7FyHd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Create Sparse Model\n",
        "###parameters\n",
        "**batch_size** :  The datasets which we use is cifar and consist of 50000 train and 10000 test images which we divide into no of batches with each batch size equal to 64 and total no of batches forms 1 epoch"
      ]
    },
    {
      "metadata": {
        "id": "y511YpgB6C0S",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def SparseNet(input_shape):\n",
        "    \n",
        "    inputs = Input(shape=input_shape)\n",
        "    \n",
        "    output = create_dense_net(inputs)\n",
        "        \n",
        "    # Create model.\n",
        "    model = Model(inputs, output)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wGFtCLK96NJl",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 6953
        },
        "outputId": "abaf4e96-cbfd-4016-b2ed-9148b0814e45",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1530371637221,
          "user_tz": -330,
          "elapsed": 6523,
          "user": {
            "displayName": "toniq apps",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "108421831546381233222"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "model = SparseNet((None,None,nb_channels))\n",
        "print(\"Model created\")\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model created\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_3 (InputLayer)            (None, None, None, 3 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_79 (Conv2D)              (None, None, None, 4 1296        input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_79 (BatchNo (None, None, None, 4 192         conv2d_79[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_79 (Activation)      (None, None, None, 4 0           batch_normalization_79[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_80 (Conv2D)              (None, None, None, 2 10368       activation_79[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_73 (Concatenate)    (None, None, None, 7 0           conv2d_80[0][0]                  \n",
            "                                                                 conv2d_79[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_80 (BatchNo (None, None, None, 7 288         concatenate_73[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_80 (Activation)      (None, None, None, 7 0           batch_normalization_80[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_81 (Conv2D)              (None, None, None, 2 15552       activation_80[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_74 (Concatenate)    (None, None, None, 4 0           conv2d_81[0][0]                  \n",
            "                                                                 conv2d_80[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_81 (BatchNo (None, None, None, 4 192         concatenate_74[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_81 (Activation)      (None, None, None, 4 0           batch_normalization_81[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_82 (Conv2D)              (None, None, None, 2 10368       activation_81[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_75 (Concatenate)    (None, None, None, 9 0           conv2d_82[0][0]                  \n",
            "                                                                 conv2d_81[0][0]                  \n",
            "                                                                 conv2d_79[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_82 (BatchNo (None, None, None, 9 384         concatenate_75[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_82 (Activation)      (None, None, None, 9 0           batch_normalization_82[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_83 (Conv2D)              (None, None, None, 2 20736       activation_82[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_76 (Concatenate)    (None, None, None, 7 0           conv2d_83[0][0]                  \n",
            "                                                                 conv2d_82[0][0]                  \n",
            "                                                                 conv2d_80[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_83 (BatchNo (None, None, None, 7 288         concatenate_76[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_83 (Activation)      (None, None, None, 7 0           batch_normalization_83[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_84 (Conv2D)              (None, None, None, 2 15552       activation_83[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_77 (Concatenate)    (None, None, None, 7 0           conv2d_84[0][0]                  \n",
            "                                                                 conv2d_83[0][0]                  \n",
            "                                                                 conv2d_81[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_84 (BatchNo (None, None, None, 7 288         concatenate_77[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_84 (Activation)      (None, None, None, 7 0           batch_normalization_84[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_85 (Conv2D)              (None, None, None, 2 15552       activation_84[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_78 (Concatenate)    (None, None, None, 7 0           conv2d_85[0][0]                  \n",
            "                                                                 conv2d_84[0][0]                  \n",
            "                                                                 conv2d_82[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_85 (BatchNo (None, None, None, 7 288         concatenate_78[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_85 (Activation)      (None, None, None, 7 0           batch_normalization_85[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_86 (Conv2D)              (None, None, None, 2 15552       activation_85[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_79 (Concatenate)    (None, None, None, 1 0           conv2d_86[0][0]                  \n",
            "                                                                 conv2d_85[0][0]                  \n",
            "                                                                 conv2d_83[0][0]                  \n",
            "                                                                 conv2d_79[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_86 (BatchNo (None, None, None, 1 480         concatenate_79[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_86 (Activation)      (None, None, None, 1 0           batch_normalization_86[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_87 (Conv2D)              (None, None, None, 2 25920       activation_86[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_80 (Concatenate)    (None, None, None, 9 0           conv2d_87[0][0]                  \n",
            "                                                                 conv2d_86[0][0]                  \n",
            "                                                                 conv2d_84[0][0]                  \n",
            "                                                                 conv2d_80[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_87 (BatchNo (None, None, None, 9 384         concatenate_80[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_87 (Activation)      (None, None, None, 9 0           batch_normalization_87[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_88 (Conv2D)              (None, None, None, 2 20736       activation_87[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_81 (Concatenate)    (None, None, None, 9 0           conv2d_88[0][0]                  \n",
            "                                                                 conv2d_87[0][0]                  \n",
            "                                                                 conv2d_85[0][0]                  \n",
            "                                                                 conv2d_81[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_88 (BatchNo (None, None, None, 9 384         concatenate_81[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_88 (Activation)      (None, None, None, 9 0           batch_normalization_88[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_89 (Conv2D)              (None, None, None, 2 20736       activation_88[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_82 (Concatenate)    (None, None, None, 9 0           conv2d_89[0][0]                  \n",
            "                                                                 conv2d_88[0][0]                  \n",
            "                                                                 conv2d_86[0][0]                  \n",
            "                                                                 conv2d_82[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_89 (BatchNo (None, None, None, 9 384         concatenate_82[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_89 (Activation)      (None, None, None, 9 0           batch_normalization_89[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_90 (Conv2D)              (None, None, None, 2 20736       activation_89[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_83 (Concatenate)    (None, None, None, 9 0           conv2d_90[0][0]                  \n",
            "                                                                 conv2d_89[0][0]                  \n",
            "                                                                 conv2d_87[0][0]                  \n",
            "                                                                 conv2d_83[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_90 (BatchNo (None, None, None, 9 384         concatenate_83[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_90 (Activation)      (None, None, None, 9 0           batch_normalization_90[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_91 (Conv2D)              (None, None, None, 2 20736       activation_90[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_84 (Concatenate)    (None, None, None, 9 0           conv2d_91[0][0]                  \n",
            "                                                                 conv2d_90[0][0]                  \n",
            "                                                                 conv2d_88[0][0]                  \n",
            "                                                                 conv2d_84[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_91 (BatchNo (None, None, None, 9 384         concatenate_84[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_91 (Activation)      (None, None, None, 9 0           batch_normalization_91[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_92 (Conv2D)              (None, None, None, 9 9216        activation_91[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_5 (AveragePoo (None, None, None, 9 0           conv2d_92[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_92 (BatchNo (None, None, None, 9 384         average_pooling2d_5[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_92 (Activation)      (None, None, None, 9 0           batch_normalization_92[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_93 (Conv2D)              (None, None, None, 2 20736       activation_92[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_85 (Concatenate)    (None, None, None, 1 0           conv2d_93[0][0]                  \n",
            "                                                                 average_pooling2d_5[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_93 (BatchNo (None, None, None, 1 480         concatenate_85[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_93 (Activation)      (None, None, None, 1 0           batch_normalization_93[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_94 (Conv2D)              (None, None, None, 2 25920       activation_93[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_86 (Concatenate)    (None, None, None, 4 0           conv2d_94[0][0]                  \n",
            "                                                                 conv2d_93[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_94 (BatchNo (None, None, None, 4 192         concatenate_86[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_94 (Activation)      (None, None, None, 4 0           batch_normalization_94[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_95 (Conv2D)              (None, None, None, 2 10368       activation_94[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_87 (Concatenate)    (None, None, None, 1 0           conv2d_95[0][0]                  \n",
            "                                                                 conv2d_94[0][0]                  \n",
            "                                                                 average_pooling2d_5[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_95 (BatchNo (None, None, None, 1 576         concatenate_87[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_95 (Activation)      (None, None, None, 1 0           batch_normalization_95[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_96 (Conv2D)              (None, None, None, 2 31104       activation_95[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_88 (Concatenate)    (None, None, None, 7 0           conv2d_96[0][0]                  \n",
            "                                                                 conv2d_95[0][0]                  \n",
            "                                                                 conv2d_93[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_96 (BatchNo (None, None, None, 7 288         concatenate_88[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_96 (Activation)      (None, None, None, 7 0           batch_normalization_96[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_97 (Conv2D)              (None, None, None, 2 15552       activation_96[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_89 (Concatenate)    (None, None, None, 7 0           conv2d_97[0][0]                  \n",
            "                                                                 conv2d_96[0][0]                  \n",
            "                                                                 conv2d_94[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_97 (BatchNo (None, None, None, 7 288         concatenate_89[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_97 (Activation)      (None, None, None, 7 0           batch_normalization_97[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_98 (Conv2D)              (None, None, None, 2 15552       activation_97[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_90 (Concatenate)    (None, None, None, 7 0           conv2d_98[0][0]                  \n",
            "                                                                 conv2d_97[0][0]                  \n",
            "                                                                 conv2d_95[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_98 (BatchNo (None, None, None, 7 288         concatenate_90[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_98 (Activation)      (None, None, None, 7 0           batch_normalization_98[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_99 (Conv2D)              (None, None, None, 2 15552       activation_98[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_91 (Concatenate)    (None, None, None, 1 0           conv2d_99[0][0]                  \n",
            "                                                                 conv2d_98[0][0]                  \n",
            "                                                                 conv2d_96[0][0]                  \n",
            "                                                                 average_pooling2d_5[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_99 (BatchNo (None, None, None, 1 672         concatenate_91[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_99 (Activation)      (None, None, None, 1 0           batch_normalization_99[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_100 (Conv2D)             (None, None, None, 2 36288       activation_99[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_92 (Concatenate)    (None, None, None, 9 0           conv2d_100[0][0]                 \n",
            "                                                                 conv2d_99[0][0]                  \n",
            "                                                                 conv2d_97[0][0]                  \n",
            "                                                                 conv2d_93[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_100 (BatchN (None, None, None, 9 384         concatenate_92[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_100 (Activation)     (None, None, None, 9 0           batch_normalization_100[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_101 (Conv2D)             (None, None, None, 2 20736       activation_100[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_93 (Concatenate)    (None, None, None, 9 0           conv2d_101[0][0]                 \n",
            "                                                                 conv2d_100[0][0]                 \n",
            "                                                                 conv2d_98[0][0]                  \n",
            "                                                                 conv2d_94[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_101 (BatchN (None, None, None, 9 384         concatenate_93[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_101 (Activation)     (None, None, None, 9 0           batch_normalization_101[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_102 (Conv2D)             (None, None, None, 2 20736       activation_101[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_94 (Concatenate)    (None, None, None, 9 0           conv2d_102[0][0]                 \n",
            "                                                                 conv2d_101[0][0]                 \n",
            "                                                                 conv2d_99[0][0]                  \n",
            "                                                                 conv2d_95[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_102 (BatchN (None, None, None, 9 384         concatenate_94[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_102 (Activation)     (None, None, None, 9 0           batch_normalization_102[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_103 (Conv2D)             (None, None, None, 2 20736       activation_102[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_95 (Concatenate)    (None, None, None, 9 0           conv2d_103[0][0]                 \n",
            "                                                                 conv2d_102[0][0]                 \n",
            "                                                                 conv2d_100[0][0]                 \n",
            "                                                                 conv2d_96[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_103 (BatchN (None, None, None, 9 384         concatenate_95[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_103 (Activation)     (None, None, None, 9 0           batch_normalization_103[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_104 (Conv2D)             (None, None, None, 2 20736       activation_103[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_96 (Concatenate)    (None, None, None, 9 0           conv2d_104[0][0]                 \n",
            "                                                                 conv2d_103[0][0]                 \n",
            "                                                                 conv2d_101[0][0]                 \n",
            "                                                                 conv2d_97[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_104 (BatchN (None, None, None, 9 384         concatenate_96[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_104 (Activation)     (None, None, None, 9 0           batch_normalization_104[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_105 (Conv2D)             (None, None, None, 9 9216        activation_104[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_6 (AveragePoo (None, None, None, 9 0           conv2d_105[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_105 (BatchN (None, None, None, 9 384         average_pooling2d_6[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_105 (Activation)     (None, None, None, 9 0           batch_normalization_105[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_106 (Conv2D)             (None, None, None, 2 20736       activation_105[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_97 (Concatenate)    (None, None, None, 1 0           conv2d_106[0][0]                 \n",
            "                                                                 average_pooling2d_6[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_106 (BatchN (None, None, None, 1 480         concatenate_97[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_106 (Activation)     (None, None, None, 1 0           batch_normalization_106[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_107 (Conv2D)             (None, None, None, 2 25920       activation_106[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_98 (Concatenate)    (None, None, None, 4 0           conv2d_107[0][0]                 \n",
            "                                                                 conv2d_106[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_107 (BatchN (None, None, None, 4 192         concatenate_98[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_107 (Activation)     (None, None, None, 4 0           batch_normalization_107[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_108 (Conv2D)             (None, None, None, 2 10368       activation_107[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_99 (Concatenate)    (None, None, None, 1 0           conv2d_108[0][0]                 \n",
            "                                                                 conv2d_107[0][0]                 \n",
            "                                                                 average_pooling2d_6[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_108 (BatchN (None, None, None, 1 576         concatenate_99[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_108 (Activation)     (None, None, None, 1 0           batch_normalization_108[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_109 (Conv2D)             (None, None, None, 2 31104       activation_108[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_100 (Concatenate)   (None, None, None, 7 0           conv2d_109[0][0]                 \n",
            "                                                                 conv2d_108[0][0]                 \n",
            "                                                                 conv2d_106[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_109 (BatchN (None, None, None, 7 288         concatenate_100[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_109 (Activation)     (None, None, None, 7 0           batch_normalization_109[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_110 (Conv2D)             (None, None, None, 2 15552       activation_109[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_101 (Concatenate)   (None, None, None, 7 0           conv2d_110[0][0]                 \n",
            "                                                                 conv2d_109[0][0]                 \n",
            "                                                                 conv2d_107[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_110 (BatchN (None, None, None, 7 288         concatenate_101[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_110 (Activation)     (None, None, None, 7 0           batch_normalization_110[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_111 (Conv2D)             (None, None, None, 2 15552       activation_110[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_102 (Concatenate)   (None, None, None, 7 0           conv2d_111[0][0]                 \n",
            "                                                                 conv2d_110[0][0]                 \n",
            "                                                                 conv2d_108[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_111 (BatchN (None, None, None, 7 288         concatenate_102[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_111 (Activation)     (None, None, None, 7 0           batch_normalization_111[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_112 (Conv2D)             (None, None, None, 2 15552       activation_111[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_103 (Concatenate)   (None, None, None, 1 0           conv2d_112[0][0]                 \n",
            "                                                                 conv2d_111[0][0]                 \n",
            "                                                                 conv2d_109[0][0]                 \n",
            "                                                                 average_pooling2d_6[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_112 (BatchN (None, None, None, 1 672         concatenate_103[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_112 (Activation)     (None, None, None, 1 0           batch_normalization_112[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_113 (Conv2D)             (None, None, None, 2 36288       activation_112[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_104 (Concatenate)   (None, None, None, 9 0           conv2d_113[0][0]                 \n",
            "                                                                 conv2d_112[0][0]                 \n",
            "                                                                 conv2d_110[0][0]                 \n",
            "                                                                 conv2d_106[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_113 (BatchN (None, None, None, 9 384         concatenate_104[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_113 (Activation)     (None, None, None, 9 0           batch_normalization_113[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_114 (Conv2D)             (None, None, None, 2 20736       activation_113[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_105 (Concatenate)   (None, None, None, 9 0           conv2d_114[0][0]                 \n",
            "                                                                 conv2d_113[0][0]                 \n",
            "                                                                 conv2d_111[0][0]                 \n",
            "                                                                 conv2d_107[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_114 (BatchN (None, None, None, 9 384         concatenate_105[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_114 (Activation)     (None, None, None, 9 0           batch_normalization_114[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_115 (Conv2D)             (None, None, None, 2 20736       activation_114[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_106 (Concatenate)   (None, None, None, 9 0           conv2d_115[0][0]                 \n",
            "                                                                 conv2d_114[0][0]                 \n",
            "                                                                 conv2d_112[0][0]                 \n",
            "                                                                 conv2d_108[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_115 (BatchN (None, None, None, 9 384         concatenate_106[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_115 (Activation)     (None, None, None, 9 0           batch_normalization_115[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_116 (Conv2D)             (None, None, None, 2 20736       activation_115[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_107 (Concatenate)   (None, None, None, 9 0           conv2d_116[0][0]                 \n",
            "                                                                 conv2d_115[0][0]                 \n",
            "                                                                 conv2d_113[0][0]                 \n",
            "                                                                 conv2d_109[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_116 (BatchN (None, None, None, 9 384         concatenate_107[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_116 (Activation)     (None, None, None, 9 0           batch_normalization_116[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_117 (Conv2D)             (None, None, None, 2 20736       activation_116[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_108 (Concatenate)   (None, None, None, 9 0           conv2d_117[0][0]                 \n",
            "                                                                 conv2d_116[0][0]                 \n",
            "                                                                 conv2d_114[0][0]                 \n",
            "                                                                 conv2d_110[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_117 (BatchN (None, None, None, 9 384         concatenate_108[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_117 (Activation)     (None, None, None, 9 0           batch_normalization_117[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_3 (Glo (None, 96)           0           activation_117[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 100)          9700        global_average_pooling2d_3[0][0] \n",
            "==================================================================================================\n",
            "Total params: 764,500\n",
            "Trainable params: 757,252\n",
            "Non-trainable params: 7,248\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "G5n_i7yUGM1B",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<b>Image Augmentation</b> If we consider example of dog image in given dataset and all dogs are facing left side then the model may not recognize all the dog facing right side after it its trained for good accuracy which may lead to overfiitting. To reduce over fitting and to improve accuracy we use image augmentation which genrates images from given datasets and produces more images with different angle and flips for better results and also we resize the images into smaller size  as the layers in the intial stages perfoms same pattern detection irrelevant of image size, therfore by reducing the size we can train our model faster and using less computation and then apply the result from this layers to images with large size"
      ]
    },
    {
      "metadata": {
        "id": "z5YsLOYD9aWe",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "8360dfc9-21e8-4a38-f9d9-735e23da52d1",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1530371639501,
          "user_tz": -330,
          "elapsed": 2214,
          "user": {
            "displayName": "toniq apps",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "108421831546381233222"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "generator = ImageDataGenerator(width_shift_range=5. / 32,\n",
        "                               height_shift_range=5. / 32,\n",
        "                               horizontal_flip=True)\n",
        "\n",
        "generator.fit(trainX, seed=0)\n",
        "\n",
        "#Plot \n",
        "# Configure batch size and retrieve one batch of imagesxwd\n",
        "for X_batch, y_batch in generator.flow(trainX, trainY, batch_size=9):\n",
        "    # Show 9 images\n",
        "    for i in range(0, 9):\n",
        "        plt.subplot(330 + 1 + i)\n",
        "        plt.imshow(toimage(X_batch[i].reshape(32, 32, 3)))\n",
        "    # show the plot\n",
        "    plt.show()\n",
        "    break"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbMAAAFNCAYAAACKdYHuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f6fbe761be0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "8WDGYRR1CA0U",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Cyclic Learning Rate\n",
        " <div align=\"justify\">This callback implements a cyclical learning rate policy (CLR).\n",
        "    The method cycles the learning rate between two boundaries with\n",
        "    some constant frequency, as detailed in this paper (https://arxiv.org/abs/1506.01186).\n",
        "    The amplitude of the cycle can be scaled on a per-iteration or per-cycle basis.\n",
        "    This class has three built-in policies, as put forth in the paper.\n",
        "    <br><br>\n",
        "    1. <b>triangular</b>: A basic triangular cycle w/ no amplitude scaling.<br>\n",
        "    2. <b>triangular2</b>: A basic triangular cycle that scales initial amplitude by half each cycle.<br>\n",
        "    3. <b>exp_range</b>: A cycle that scales initial amplitude by gamma (cycle iterations) at each \n",
        "        cycle iteration.<br></div>"
      ]
    },
    {
      "metadata": {
        "id": "dYeTXj2cYaS7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Arguments\n",
        "  <br><b> base_lr</b>: initial learning rate which is the lower boundary in the cycle.\n",
        "  <br><b>max_lr</b>: upper boundary in the cycle. Functionally, it defines the cycle amplitude (max_lr - base_lr). The lr at any cycle is the sum of base_lr and some scaling of the amplitude; therefore max_lr may not actually be reached depending on scaling function.\n",
        "  <br><b>step_size</b>: number of training iterations per half cycle. Authors suggest setting step_size 2-8 x training iterations in epoch.\n",
        "  <br><b>mode</b>: one of {triangular, triangular2, exp_range}. Default 'triangular'.  Values correspond to policies detailed above. If scale_fn is not None, this argument is ignored. \n",
        "  <br><b>gamma</b>: constant in 'exp_range' scaling function:  gamma**(cycle iterations)\n",
        "  <br><b>scale_fn</b>: Custom scaling policy defined by a single argument lambda function, where 0 <= scale_fn(x) <= 1 for all x >= 0. mode paramater is ignored  \n",
        "  <b>scale_mode:</b> {'cycle', 'iterations'}. Defines whether scale_fn is evaluated on cycle number or cycle iterations (training iterations since start of cycle). Default is 'cycle'.</div>"
      ]
    },
    {
      "metadata": {
        "id": "L7PH0WmrkduC",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.callbacks import *\n",
        "class CyclicLR(Callback):\n",
        "    def __init__(self, base_lr=0.001, max_lr=0.006, step_size=2000, mode='triangular', gamma=1., scale_fn=None, scale_mode='cycle'):\n",
        "        super(CyclicLR, self).__init__()\n",
        "\n",
        "        self.base_lr = base_lr\n",
        "        self.max_lr = max_lr\n",
        "        self.step_size = step_size\n",
        "        self.mode = mode\n",
        "        self.gamma = gamma\n",
        "        if scale_fn == None:\n",
        "            if self.mode == 'triangular':\n",
        "                self.scale_fn = lambda x: 1.\n",
        "                self.scale_mode = 'cycle'\n",
        "            elif self.mode == 'triangular2':\n",
        "                self.scale_fn = lambda x: 1/(2.**(x-1))\n",
        "                self.scale_mode = 'cycle'\n",
        "            elif self.mode == 'exp_range':\n",
        "                self.scale_fn = lambda x: gamma**(x)\n",
        "                self.scale_mode = 'iterations'\n",
        "        else:\n",
        "            self.scale_fn = scale_fn\n",
        "            self.scale_mode = scale_mode\n",
        "        self.clr_iterations = 0.\n",
        "        self.trn_iterations = 0.\n",
        "        self.history = {}\n",
        "\n",
        "        self._reset()\n",
        "\n",
        "    def _reset(self, new_base_lr=None, new_max_lr=None,\n",
        "               new_step_size=None):\n",
        "        \"\"\"Resets cycle iterations.\n",
        "        Optional boundary/step size adjustment.\n",
        "        \"\"\"\n",
        "        if new_base_lr != None:\n",
        "            self.base_lr = new_base_lr\n",
        "        if new_max_lr != None:\n",
        "            self.max_lr = new_max_lr\n",
        "        if new_step_size != None:\n",
        "            self.step_size = new_step_size\n",
        "        self.clr_iterations = 0.\n",
        "        \n",
        "    def clr(self):\n",
        "        cycle = np.floor(1+self.clr_iterations/(2*self.step_size))\n",
        "        x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)\n",
        "        if self.scale_mode == 'cycle':\n",
        "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)\n",
        "        else:\n",
        "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)\n",
        "        \n",
        "    def on_train_begin(self, logs={}):\n",
        "        logs = logs or {}\n",
        "\n",
        "        if self.clr_iterations == 0:\n",
        "            K.set_value(self.model.optimizer.lr, self.base_lr)\n",
        "        else:\n",
        "            K.set_value(self.model.optimizer.lr, self.clr())        \n",
        "            \n",
        "    def on_batch_end(self, epoch, logs=None):\n",
        "        \n",
        "        logs = logs or {}\n",
        "        self.trn_iterations += 1\n",
        "        self.clr_iterations += 1\n",
        "\n",
        "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
        "        self.history.setdefault('iterations', []).append(self.trn_iterations)\n",
        "\n",
        "        for k, v in logs.items():\n",
        "            self.history.setdefault(k, []).append(v)\n",
        "        \n",
        "        K.set_value(self.model.optimizer.lr, self.clr())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Uw60h0gec1LC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Experiment\n",
        "\n",
        "<div align=\"justify\">**Objective:** Create a model using <b><i>Sparsenet</i></b> architecture and train the model on <i><b>CIFAR100</b></i> dataset</div>\n",
        "\n",
        "<div align=\"justify\">We use densnet architecture for our model and replace the concatenation part with exponential $2^N$ function keeping all the hyper parameters and model architecture same. As i have already explained why sparse net exponential term has advantage over denset and resnet i would like to present the facts in below experiment\n",
        "  \n",
        " We use <i><b>Stochastic Gradient Descent (SGD)</b></i> optimizer. I have set the learning rate as 0.1 and we have also use momentum parameter which helps accelerate SGD in right direction. During the experiment i used  3 methods to set learning rate during training  \n",
        " 1. <b>Time-Based Learning Rate Schedule</b> in which we decay the learning rate by certain value at each epoch let say $$\\\\text{decay} = \\text{learning rate} /\\text{ no of epoch}$$\n",
        " <br>\n",
        " 2.<b> Drop-Based Learning Rate Schedule</b> in which we reduce learning rate after certain epochs for example in densnet paper the initial lr=0.1 and reduce to 0.01 when epoch reaches 50% and 0.001 when epoch reaches to 75% \n",
        " \n",
        "Both the above method are form of <i><b>Adaptive Learning Rate</b></i> in which we reduce learning rate linearly but in both cases what should be the base learning rate since if learning rate is to low it lead to slow convergence and if it is high it may lead to divergence and we may need to train model with diifferent learning rate to find the optimal result\n",
        "</br></br>\n",
        "3.  <b>Cyclic Learning Rate</b> : To overcome the above issue  we can use cyclic learning rate with lr range between min and max. This method instead of reducing learning rate linearly it changes the learning rate between the given bound i.e. the range between max and min lr for given step size. But how to find the min and max lr bound. To find the bound we use <b><i>Learning rate range calculator</i></b> where we set the min lr and max lr and step size equal to total no of iteration and and train our model by increasing the the learning rate and then plot the accuracy against the  learning rate and find the point were the accuracy is increased to max and then start decreasing but we never consider the max point since from the point onwards the accuracy starts reducing so we consider learning rate previous to max accuracy over learning rate.  Alternatively, one can use the rule of thumb that the optimum learning rate is usually within a factor of two of the largest one that converges and set base lr to 1/3 or 1/4 of max lr. Other advantage of cyclic learning rate is it requires less iteration compared to model without clr to achieve the same accuracy\n",
        "</div>\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "JFB1Hn0OBf85",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "batch_size = 100\n",
        "nb_epoch = 100\n",
        "weights_file = 'SparseNet-40-24-CIFAR100_CLR.h5';"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pK0T9slc9wG_",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=SGD(lr=0.1,decay=0.0, momentum=0.9,nesterov=True), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model_checkpoint = ModelCheckpoint(weights_file, monitor=\"val_acc\", save_best_only=True, save_weights_only=True, verbose=1)\n",
        "cyclic_lr = CyclicLR(mode='triangular')\n",
        "callbacks = [cyclic_lr, model_checkpoint]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EAlQio80hmsb",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def graph_accuracy(history):\n",
        "  # list all data in history\n",
        "  print(history.history.keys())\n",
        "  # summarize history for accuracy\n",
        "  plt.plot(history.history['acc'])\n",
        "  plt.plot(history.history['val_acc'])\n",
        "  plt.title('model accuracy')\n",
        "  plt.ylabel('accuracy')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'test'], loc='upper left')\n",
        "  plt.show()\n",
        "  # summarize history for loss\n",
        "  plt.plot(history.history['loss'])\n",
        "  plt.plot(history.history['val_loss'])\n",
        "  plt.title('model loss')\n",
        "  plt.ylabel('loss')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'test'], loc='upper left')\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "s-h9Gk3mo9Hq",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#due to shortage of time and freequent discconect i was not able to train the model from start but i was able save the model weights which i am reloading\n",
        "#pretrained model of of above model with image size 24 for 30 epoch using SGD optimizer with learning rate = 0.1, decay = 0.0 and momentum =0.9\n",
        "'''#size 24\n",
        "nb_epoch =15\n",
        "trainX_24 = [imresize(image, (24, 24, 3)) for image in trainX]\n",
        "trainX_24 = np.array(trainX)\n",
        "\n",
        "history_24 = model.fit_generator(generator.flow(trainX_24, Y_train, batch_size),\n",
        "                    steps_per_epoch=len(trainX_24) // batch_size, epochs=nb_epoch,\n",
        "                    callbacks=callbacks,\n",
        "                    validation_data=(testX, Y_test),\n",
        "                    validation_steps=testX.shape[0] // batch_size, verbose=1)'''\n",
        "\n",
        "\n",
        "from keras.utils.data_utils import get_file\n",
        "weights_path = get_file(\n",
        "            'SparseNet-40-24-CIFAR100_CLR_30',\n",
        "            'https://drive.google.com/uc?export=download&id=1KSEVaVuxKCMdSgNueKxktBziDIcsU9xO')\n",
        "model.load_weights(weights_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CV60oQ0e-aja",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 4365
        },
        "outputId": "8a65dbcb-ebd9-47b5-e01a-4c76e9633334",
        "executionInfo": {
          "status": "error",
          "timestamp": 1530380599917,
          "user_tz": -330,
          "elapsed": 8948943,
          "user": {
            "displayName": "toniq apps",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "108421831546381233222"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "nb_epoch = 50\n",
        "#image size 32,32\n",
        "history_32 = model.fit_generator(generator.flow(trainX, Y_train, batch_size=batch_size),\n",
        "                    steps_per_epoch=len(trainX) // batch_size, epochs=nb_epoch,\n",
        "                    callbacks=callbacks,\n",
        "                    validation_data=(testX, Y_test),\n",
        "                    validation_steps=testX.shape[0] // batch_size, verbose=1)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "500/500 [==============================] - 212s 423ms/step - loss: 0.8704 - acc: 0.7604 - val_loss: 1.3478 - val_acc: 0.6559\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.65590, saving model to SparseNet-40-24-CIFAR100_CLR.h5\n",
            "Epoch 2/50\n",
            "162/500 [========>.....................] - ETA: 2:08 - loss: 0.8378 - acc: 0.7713"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500/500 [==============================] - 206s 411ms/step - loss: 0.8086 - acc: 0.7796 - val_loss: 1.3421 - val_acc: 0.6596\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.65590 to 0.65960, saving model to SparseNet-40-24-CIFAR100_CLR.h5\n",
            "Epoch 3/50\n",
            "224/500 [============>.................] - ETA: 1:45 - loss: 0.7798 - acc: 0.7869"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500/500 [==============================] - 205s 411ms/step - loss: 0.7736 - acc: 0.7897 - val_loss: 1.3066 - val_acc: 0.6640\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.65960 to 0.66400, saving model to SparseNet-40-24-CIFAR100_CLR.h5\n",
            "Epoch 4/50\n",
            "240/500 [=============>................] - ETA: 1:39 - loss: 0.7583 - acc: 0.7949"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500/500 [==============================] - 205s 410ms/step - loss: 0.7500 - acc: 0.7977 - val_loss: 1.2969 - val_acc: 0.6702\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.66400 to 0.67020, saving model to SparseNet-40-24-CIFAR100_CLR.h5\n",
            "Epoch 5/50\n",
            "244/500 [=============>................] - ETA: 1:37 - loss: 0.7317 - acc: 0.8018"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500/500 [==============================] - 205s 411ms/step - loss: 0.7281 - acc: 0.8032 - val_loss: 1.3089 - val_acc: 0.6686\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.67020\n",
            "Epoch 6/50\n",
            "295/500 [================>.............] - ETA: 1:18 - loss: 0.7138 - acc: 0.8094"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500/500 [==============================] - 205s 410ms/step - loss: 0.7171 - acc: 0.8065 - val_loss: 1.3128 - val_acc: 0.6717\n",
            "\n",
            "Epoch 00006: val_acc improved from 0.67020 to 0.67170, saving model to SparseNet-40-24-CIFAR100_CLR.h5\n",
            "Epoch 7/50\n",
            "258/500 [==============>...............] - ETA: 1:32 - loss: 0.7037 - acc: 0.8105"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500/500 [==============================] - 205s 411ms/step - loss: 0.7077 - acc: 0.8085 - val_loss: 1.2977 - val_acc: 0.6775\n",
            "\n",
            "Epoch 00007: val_acc improved from 0.67170 to 0.67750, saving model to SparseNet-40-24-CIFAR100_CLR.h5\n",
            "Epoch 8/50\n",
            "249/500 [=============>................] - ETA: 1:35 - loss: 0.6942 - acc: 0.8098"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500/500 [==============================] - 205s 410ms/step - loss: 0.7021 - acc: 0.8074 - val_loss: 1.3130 - val_acc: 0.6758\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.67750\n",
            "Epoch 9/50\n",
            "296/500 [================>.............] - ETA: 1:17 - loss: 0.7006 - acc: 0.8113"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500/500 [==============================] - 205s 410ms/step - loss: 0.6973 - acc: 0.8110 - val_loss: 1.3174 - val_acc: 0.6739\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.67750\n",
            "Epoch 10/50\n",
            "309/500 [=================>............] - ETA: 1:12 - loss: 0.6949 - acc: 0.8123"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500/500 [==============================] - 205s 411ms/step - loss: 0.6963 - acc: 0.8123 - val_loss: 1.2814 - val_acc: 0.6760\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.67750\n",
            "Epoch 11/50\n",
            "313/500 [=================>............] - ETA: 1:11 - loss: 0.6931 - acc: 0.8086"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500/500 [==============================] - 205s 410ms/step - loss: 0.6931 - acc: 0.8084 - val_loss: 1.3047 - val_acc: 0.6745\n",
            "\n",
            "Epoch 00011: val_acc did not improve from 0.67750\n",
            "Epoch 12/50\n",
            "314/500 [=================>............] - ETA: 1:10 - loss: 0.6858 - acc: 0.8131"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500/500 [==============================] - 205s 410ms/step - loss: 0.6878 - acc: 0.8104 - val_loss: 1.2941 - val_acc: 0.6794\n",
            "\n",
            "Epoch 00012: val_acc improved from 0.67750 to 0.67940, saving model to SparseNet-40-24-CIFAR100_CLR.h5\n",
            "Epoch 13/50\n",
            "262/500 [==============>...............] - ETA: 1:30 - loss: 0.6905 - acc: 0.8124"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500/500 [==============================] - 205s 410ms/step - loss: 0.6849 - acc: 0.8118 - val_loss: 1.3254 - val_acc: 0.6731\n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.67940\n",
            "Epoch 14/50\n",
            "299/500 [================>.............] - ETA: 1:16 - loss: 0.6782 - acc: 0.8149"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500/500 [==============================] - 205s 411ms/step - loss: 0.6753 - acc: 0.8154 - val_loss: 1.3097 - val_acc: 0.6786\n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.67940\n",
            "Epoch 15/50\n",
            "310/500 [=================>............] - ETA: 1:12 - loss: 0.6551 - acc: 0.8203"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500/500 [==============================] - 205s 409ms/step - loss: 0.6637 - acc: 0.8184 - val_loss: 1.2906 - val_acc: 0.6833\n",
            "\n",
            "Epoch 00015: val_acc improved from 0.67940 to 0.68330, saving model to SparseNet-40-24-CIFAR100_CLR.h5\n",
            "Epoch 16/50\n",
            "261/500 [==============>...............] - ETA: 1:30 - loss: 0.6647 - acc: 0.8197"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500/500 [==============================] - 205s 409ms/step - loss: 0.6660 - acc: 0.8178 - val_loss: 1.2853 - val_acc: 0.6827\n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.68330\n",
            "Epoch 17/50\n",
            "299/500 [================>.............] - ETA: 1:16 - loss: 0.6535 - acc: 0.8205"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500/500 [==============================] - 205s 410ms/step - loss: 0.6574 - acc: 0.8196 - val_loss: 1.3007 - val_acc: 0.6763\n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.68330\n",
            "Epoch 18/50\n",
            "310/500 [=================>............] - ETA: 1:12 - loss: 0.6613 - acc: 0.8184"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500/500 [==============================] - 205s 409ms/step - loss: 0.6585 - acc: 0.8194 - val_loss: 1.3473 - val_acc: 0.6708\n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.68330\n",
            "Epoch 19/50\n",
            "314/500 [=================>............] - ETA: 1:10 - loss: 0.6603 - acc: 0.8189"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500/500 [==============================] - 205s 410ms/step - loss: 0.6650 - acc: 0.8169 - val_loss: 1.3392 - val_acc: 0.6745\n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.68330\n",
            "Epoch 20/50\n",
            "315/500 [=================>............] - ETA: 1:10 - loss: 0.6558 - acc: 0.8191"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500/500 [==============================] - 205s 410ms/step - loss: 0.6572 - acc: 0.8195 - val_loss: 1.3251 - val_acc: 0.6753\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.68330\n",
            "Epoch 21/50\n",
            "315/500 [=================>............] - ETA: 1:10 - loss: 0.6423 - acc: 0.8212"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500/500 [==============================] - 204s 409ms/step - loss: 0.6469 - acc: 0.8202 - val_loss: 1.3573 - val_acc: 0.6765\n",
            "\n",
            "Epoch 00021: val_acc did not improve from 0.68330\n",
            "Epoch 22/50\n",
            "315/500 [=================>............] - ETA: 1:10 - loss: 0.6409 - acc: 0.8206"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500/500 [==============================] - 205s 410ms/step - loss: 0.6460 - acc: 0.8202 - val_loss: 1.3115 - val_acc: 0.6827\n",
            "\n",
            "Epoch 00022: val_acc did not improve from 0.68330\n",
            "Epoch 23/50\n",
            "315/500 [=================>............] - ETA: 1:10 - loss: 0.6436 - acc: 0.8207"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500/500 [==============================] - 205s 410ms/step - loss: 0.6412 - acc: 0.8232 - val_loss: 1.3277 - val_acc: 0.6731\n",
            "\n",
            "Epoch 00023: val_acc did not improve from 0.68330\n",
            "Epoch 24/50\n",
            "315/500 [=================>............] - ETA: 1:10 - loss: 0.6361 - acc: 0.8236"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500/500 [==============================] - 204s 409ms/step - loss: 0.6351 - acc: 0.8244 - val_loss: 1.3663 - val_acc: 0.6708\n",
            "\n",
            "Epoch 00024: val_acc did not improve from 0.68330\n",
            "Epoch 25/50\n",
            "315/500 [=================>............] - ETA: 1:10 - loss: 0.6384 - acc: 0.8251"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500/500 [==============================] - 205s 410ms/step - loss: 0.6333 - acc: 0.8256 - val_loss: 1.3197 - val_acc: 0.6744\n",
            "\n",
            "Epoch 00025: val_acc did not improve from 0.68330\n",
            "Epoch 26/50\n",
            "315/500 [=================>............] - ETA: 1:10 - loss: 0.6279 - acc: 0.8264"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500/500 [==============================] - 204s 409ms/step - loss: 0.6320 - acc: 0.8255 - val_loss: 1.3535 - val_acc: 0.6692\n",
            "\n",
            "Epoch 00026: val_acc did not improve from 0.68330\n",
            "Epoch 27/50\n",
            "315/500 [=================>............] - ETA: 1:10 - loss: 0.6305 - acc: 0.8265"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500/500 [==============================] - 205s 409ms/step - loss: 0.6341 - acc: 0.8251 - val_loss: 1.3450 - val_acc: 0.6760\n",
            "\n",
            "Epoch 00027: val_acc did not improve from 0.68330\n",
            "Epoch 28/50\n",
            "315/500 [=================>............] - ETA: 1:10 - loss: 0.6269 - acc: 0.8269"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500/500 [==============================] - 205s 410ms/step - loss: 0.6296 - acc: 0.8256 - val_loss: 1.3684 - val_acc: 0.6728\n",
            "\n",
            "Epoch 00028: val_acc did not improve from 0.68330\n",
            "Epoch 29/50\n",
            "315/500 [=================>............] - ETA: 1:10 - loss: 0.6260 - acc: 0.8238"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500/500 [==============================] - 205s 409ms/step - loss: 0.6273 - acc: 0.8241 - val_loss: 1.3531 - val_acc: 0.6742\n",
            "\n",
            "Epoch 00029: val_acc did not improve from 0.68330\n",
            "Epoch 30/50\n",
            "315/500 [=================>............] - ETA: 1:10 - loss: 0.6192 - acc: 0.8283"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500/500 [==============================] - 205s 410ms/step - loss: 0.6197 - acc: 0.8272 - val_loss: 1.3307 - val_acc: 0.6780\n",
            "\n",
            "Epoch 00030: val_acc did not improve from 0.68330\n",
            "Epoch 31/50\n",
            "315/500 [=================>............] - ETA: 1:10 - loss: 0.6112 - acc: 0.8305"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500/500 [==============================] - 205s 410ms/step - loss: 0.6130 - acc: 0.8305 - val_loss: 1.3119 - val_acc: 0.6833\n",
            "\n",
            "Epoch 00031: val_acc did not improve from 0.68330\n",
            "Epoch 32/50\n",
            "315/500 [=================>............] - ETA: 1:10 - loss: 0.6099 - acc: 0.8298"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500/500 [==============================] - 205s 411ms/step - loss: 0.6111 - acc: 0.8301 - val_loss: 1.3422 - val_acc: 0.6780\n",
            "\n",
            "Epoch 00032: val_acc did not improve from 0.68330\n",
            "Epoch 33/50\n",
            "315/500 [=================>............] - ETA: 1:10 - loss: 0.6147 - acc: 0.8291"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500/500 [==============================] - 205s 410ms/step - loss: 0.6092 - acc: 0.8305 - val_loss: 1.3368 - val_acc: 0.6789\n",
            "\n",
            "Epoch 00033: val_acc did not improve from 0.68330\n",
            "Epoch 34/50\n",
            "315/500 [=================>............] - ETA: 1:10 - loss: 0.6075 - acc: 0.8305"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500/500 [==============================] - 205s 410ms/step - loss: 0.6129 - acc: 0.8287 - val_loss: 1.3355 - val_acc: 0.6785\n",
            "\n",
            "Epoch 00034: val_acc did not improve from 0.68330\n",
            "Epoch 35/50\n",
            "315/500 [=================>............] - ETA: 1:10 - loss: 0.6123 - acc: 0.8295"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500/500 [==============================] - 205s 410ms/step - loss: 0.6117 - acc: 0.8288 - val_loss: 1.3478 - val_acc: 0.6752\n",
            "\n",
            "Epoch 00035: val_acc did not improve from 0.68330\n",
            "Epoch 36/50\n",
            "315/500 [=================>............] - ETA: 1:10 - loss: 0.6018 - acc: 0.8327"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500/500 [==============================] - 205s 411ms/step - loss: 0.6102 - acc: 0.8302 - val_loss: 1.3281 - val_acc: 0.6805\n",
            "\n",
            "Epoch 00036: val_acc did not improve from 0.68330\n",
            "Epoch 37/50\n",
            "315/500 [=================>............] - ETA: 1:10 - loss: 0.6051 - acc: 0.8313"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500/500 [==============================] - 204s 408ms/step - loss: 0.6066 - acc: 0.8316 - val_loss: 1.3737 - val_acc: 0.6728\n",
            "\n",
            "Epoch 00037: val_acc did not improve from 0.68330\n",
            "Epoch 38/50\n",
            "315/500 [=================>............] - ETA: 1:10 - loss: 0.5953 - acc: 0.8341"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500/500 [==============================] - 205s 410ms/step - loss: 0.6005 - acc: 0.8331 - val_loss: 1.3365 - val_acc: 0.6789\n",
            "\n",
            "Epoch 00038: val_acc did not improve from 0.68330\n",
            "Epoch 39/50\n",
            "315/500 [=================>............] - ETA: 1:10 - loss: 0.5922 - acc: 0.8344"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500/500 [==============================] - 205s 410ms/step - loss: 0.5942 - acc: 0.8340 - val_loss: 1.3752 - val_acc: 0.6717\n",
            "\n",
            "Epoch 00039: val_acc did not improve from 0.68330\n",
            "Epoch 40/50\n",
            "315/500 [=================>............] - ETA: 1:10 - loss: 0.5905 - acc: 0.8351"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500/500 [==============================] - 205s 410ms/step - loss: 0.5919 - acc: 0.8349 - val_loss: 1.3474 - val_acc: 0.6798\n",
            "\n",
            "Epoch 00040: val_acc did not improve from 0.68330\n",
            "Epoch 41/50\n",
            "315/500 [=================>............] - ETA: 1:10 - loss: 0.5919 - acc: 0.8350"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500/500 [==============================] - 205s 409ms/step - loss: 0.5906 - acc: 0.8349 - val_loss: 1.3553 - val_acc: 0.6753\n",
            "\n",
            "Epoch 00041: val_acc did not improve from 0.68330\n",
            "Epoch 42/50\n",
            "315/500 [=================>............] - ETA: 1:10 - loss: 0.5913 - acc: 0.8338"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500/500 [==============================] - 205s 410ms/step - loss: 0.5892 - acc: 0.8349 - val_loss: 1.3607 - val_acc: 0.6785\n",
            "\n",
            "Epoch 00042: val_acc did not improve from 0.68330\n",
            "Epoch 43/50\n",
            "315/500 [=================>............] - ETA: 1:10 - loss: 0.5824 - acc: 0.8358"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500/500 [==============================] - 205s 410ms/step - loss: 0.5884 - acc: 0.8343 - val_loss: 1.3708 - val_acc: 0.6740\n",
            "\n",
            "Epoch 00043: val_acc did not improve from 0.68330\n",
            "Epoch 44/50\n",
            "238/500 [=============>................] - ETA: 1:39 - loss: 0.5821 - acc: 0.8369"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-61-40955e94294f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                     validation_steps=testX.shape[0] // batch_size, verbose=1)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2228\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   2229\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2230\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   2231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2232\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1881\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1883\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1884\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1885\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2480\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2481\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2482\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2483\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "0bALHz0B8sO1",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('SparseNet-40-24-CIFAR100_CLR.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SDboLKe62Qns",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "yPreds = model.predict(testX)\n",
        "yPred = np.argmax(yPreds, axis=1)\n",
        "yTrue = testY\n",
        "\n",
        "accuracy = metrics.accuracy_score(yTrue, yPred) * 100\n",
        "error = 100 - accuracy\n",
        "print(\"Accuracy : \", accuracy)\n",
        "print(\"Error : \", error)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}